{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Labor 3\n",
        "\n",
        "A párhuzamos algoritmusok célja a [teljesítmény növelése](https://courses.cs.washington.edu/courses/csep524/07sp/poppChaper1.pdf). Ezt azzal érik el, hogy a számításokat egyszerre több számítógépen vagy számítási egységen végzik el, így csökkentve a számítási időt, vagy lehetővé téve pontosabb eredmények elérését egy adott időegység alatt.\n",
        "\n",
        "\n",
        "# Teljesítménymértékek\n",
        "\n",
        "A teljesítménynövekedés számszerű kifejezése érdekében áttekintjük, hogy milyen módszerek állnak rendelkezésre.\n",
        "\n",
        "## Gyorsítás (speedup)\n",
        "\n",
        "Az időarány, amelyben a párhuzamos algoritmus végrehajtása gyorsabb, mint a szekvenciális algoritmusé. A gyorsítás értéke az eredeti, szekvenciális idő és az új idő hányadosa.\n",
        "\n",
        "Jelölje $T(n,p)$ azt a függvényt, ami megadja, hogy egy párhuzamos program egy $n$ méretű feladatot mennyi idő alatt old meg $p$ processzor használatával.\n",
        "\n",
        "Adott $n$-méretű feladatra a $T(n,p)$ párhuzamos $p$ processzoron futó program reális  **gyorsítását (gyorsítási faktort)**\n",
        "\n",
        "$S(n,p)=\\frac{T^*(n)}{T(n,p)}$\n",
        "\n",
        "adja meg, ahol $T^*(n)$ a leggyorsabb ismert soros algoritmus futási ideje.\n",
        "\n",
        "\n",
        "\n",
        "A környezettől függően a gyorsítás vonatkozhat $p$ folyamat vagy $p$ szál alkalmazására is.\n",
        "\n",
        "$T^*(n)$ nem mindig ismert, vagy nem implementálható, ezért gyakran a relatív **_gyorsítását_**  használjuk, ahol a párhuzamos program egy processzoros futási idejét vesszük figyelembe:\n",
        "\n",
        "$S(n,p)=\\frac{T(n,1)}{T(n,p)}$\n",
        "\n",
        "\n",
        "## Hatékonyság (efficiency)\n",
        "\n",
        "Másik fontos metrika a **_párhuzamos hatékonyság_** ami kifejezi a gyorsítás arányát a felhasznált számítási erőforrásokhoz képest. Az hatékonyság értéke a gyorsulás és a felhasznált erőforrások (például a processzorok vagy a magok száma) hányadosa:\n",
        "\n",
        "$E(n,p)=\\frac{S(n,p)}{p}=\\frac{T^*(n)}{p*T(n,p)}$\n",
        "\n",
        "Ez a mutató betekintést nyújt a processzorok kihasználtságára, ideális esetben ez az érték 1 (lineáris gyorsulás).\n",
        "\n",
        "Egy párhuzamos programot akkor nevezünk **_skálázhatónak_**, ha a párhuzamos hatékonyság fenntartható. Skálázhatóság (scalability): a rendszer képessége, hogy a számítási feladatokat nagyobb erőforrásokkal bővítve hatékonyan kezelje.\n",
        "\n"
      ],
      "metadata": {
        "id": "ikVGiziWBV53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Időmérésa CUDA események segítségével\n",
        "\n",
        "A CUDA esemény ([CUDA event](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html#group__CUDART__EVENT)) egy szinkronizációs jel, amely lehetővé teszi a programozó számára, hogy monitorizálja a CUDA munkafolyamatokat, szinkronizálja a CUDA adatfolyamokat (CUDA streams) és lehetőve teszi a [CUDA kernelek pontos időmérését](https://developer.nvidia.com/blog/how-implement-performance-metrics-cuda-cc/).\n",
        "\n",
        "\n",
        "A `cudaEvent` egy CUDA API függvény, amely lehetővé teszi a programozók számára, hogy mérjék a CUDA műveletek végrehajtási idejét. A `cudaEvent` funkciók segítségével időbélyegeket állíthatunk be, majd ezen bélyegek között eltelt időt mérhetjük.\n",
        "\n",
        "A következő lépésekkel használjuk a `cudaEvent`-et időmérésre:\n",
        "\n",
        "1. Létre kell hozni két `cudaEvent` objektumot, amelyek közül az egyiket a művelet kezdeteként, a másikat pedig a művelet befejezéseként jelöljük meg. Például:\n",
        "  ```cpp\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "  ```\n",
        "2. A [`cudaEventRecord`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1gf4fcb74343aa689f4159791967868446) függvény segítségével be kell állítanunk a start és stop bélyegeket a művelet elkezdése elött és a befejezésnél. Például:\n",
        "  ```cpp\n",
        "  cudaEventRecord(start, 0);\n",
        "  // CUDA műveletek végrehajtása\n",
        "  // pl. cudaMalloc, cudaMemcpy, kernel hívás, eredmények visszamásolása, memória felszabadítás\n",
        "  cudaEventRecord(stop, 0);\n",
        "  ```\n",
        "  A második paraméter a `cudaEventRecord` hívásokban az adatfolyamot ([CUDA stream](https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf)) jelöli, és a 0 érték azt jelzi, hogy az esemény a null adatfolyamhoz van rendelve.\n",
        "\n",
        "  Az adatfolyamok (stream) olyan CUDA fogalmak, amelyek lehetővé teszik a kernel futtatását és az adatmozgatást aszinkron módon. Az adatfolyamok segítségével párhuzamosíthatjuk a műveleteket és jobban kihasználhatjuk a hardverünk párhuzamosítási képességeit.\n",
        "\n",
        "3. A [`cudaEventSynchronize`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g949aa42b30ae9e622f6ba0787129ff22) függvény használatával megvárjuk, amíg a stop időbélyeg rögzítése megtörténik, majd ellenőrizzük az esetleges hibákat. Például:\n",
        "  ```cpp\n",
        "  cudaEventSynchronize(stop);\n",
        "  cudaError_t error = cudaGetLastError();\n",
        "  if (error != cudaSuccess) {\n",
        "      printf(\"CUDA hiba: %s\\n\", cudaGetErrorString(error));\n",
        "  }\n",
        "  ```\n",
        "\n",
        "4. Végül az [`cudaEventElapsedTime`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g40159125411db92c835edb46a0989cd6) függvény segítségével kiszámítjuk a két bélyeg között eltelt időt. Például:\n",
        "  ```cpp\n",
        "  float elapsedTime;\n",
        "  cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "  printf(\"A CUDA műveletek elvégzése %.2f ms telt.\\n\", elapsedTime);\n",
        "  ```\n",
        "5. A [`cudaEventDestroy`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html#group__CUDART__EVENT_1g2cb6baa0830a1cd0bd957bfd8705045b) függvényt használjuk az általunk létrehozott `cudaEvent` objektumok felszabadítására. Például:\n",
        "  ```cpp\n",
        "  cudaEventDestroy(start);\n",
        "  cudaEventDestroy(stop);\n",
        "  ```\n",
        "\n",
        "Az alábbi példa összefoglalja a fenti lépéseket:\n",
        "\n",
        "```cpp\n",
        "__global__ void addKernel(int *c, const int *a, const int *b, int size)\n",
        "{\n",
        "    int i = threadIdx.x;\n",
        "    if (i < size) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    const int size = 1024;\n",
        "    int a[size], b[size], c[size];\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        a[i] = i;\n",
        "        b[i] = 2 * i;\n",
        "    }\n",
        "\n",
        "    int *dev_a, *dev_b, *dev_c;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    cudaMalloc(&dev_a, size * sizeof(int));\n",
        "    cudaMalloc(&dev_b, size * sizeof(int));\n",
        "    cudaMalloc(&dev_c, size * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(dev_a, a, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(dev_b, b, size * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    addKernel<<<1, size>>>(dev_c, dev_a, dev_b, size);\n",
        "\n",
        "    cudaMemcpy(c, dev_c, size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    cudaFree(dev_a);\n",
        "    cudaFree(dev_b);\n",
        "    cudaFree(dev_c);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaError_t error = cudaGetLastError();\n",
        "    if (error != cudaSuccess) {\n",
        "        printf(\"CUDA hiba: %s\\n\", cudaGetErrorString(error));\n",
        "    }\n",
        "    float elapsedTime;\n",
        "    cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "    printf(\"A CUDA művelet %.2f millió másodpercig tartott.\\n\", elapsedTime);\n",
        "    \n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    return 0;\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "phM6n5Mt1Odq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Időmérés CPU-n\n",
        "\n",
        "C++-ban, egy nagyfelbontású aktuális időpontot, a\n",
        "```cpp\n",
        "static  std::chrono::time_point<std::chrono::high_resolution_clock> now()  noexcept;\n",
        "```\n",
        "[függvénnyel](https://en.cppreference.com/w/cpp/chrono/high_resolution_clock/now) kérhetjük le. A rendszer maximális időfelbontását (_ticks per second_) a `std::chrono::high_resolution_clock::period::den` érték lekérésével kaphatjuk meg.\n",
        "\n",
        "Ha kíváncsiak vagyunk egy kódrészlet futási idejére, megmérhetjük azt két időbélyegek segítségével, melyeket lekérdezünk mindjárt a kódrészlet előtt és után. A két időbélyeg különbsége megadja az eltelt időt, a kívánt [mértékegységben](https://en.cppreference.com/w/cpp/chrono/duration/duration_cast):\n",
        "\n",
        "![](https://i.ibb.co/F5D8brL/duration-cast.png)\n",
        "\n",
        "\n",
        "### Példa időmérésre:\n",
        "\n",
        "```cpp\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "using  namespace std;\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tcout  << chrono::high_resolution_clock::period::den  << endl;\n",
        "\tauto start_time = chrono::high_resolution_clock::now();\n",
        "\tlong temp =  0;\n",
        "\tfor  (auto i =  0; i < 100000000; i++)\n",
        "\t\ttemp += i;\n",
        "\tauto end_time = chrono::high_resolution_clock::now();\n",
        "\tcout  << chrono::duration_cast<chrono::seconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout  << chrono::duration_cast<chrono::milliseconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout  << chrono::duration_cast<chrono::microseconds>(end_time - start_time).count()  <<  \":\";\n",
        "\tcout << endl;\n",
        "\n",
        "\treturn  0;\n",
        "}\n",
        "```\n",
        "Mivel ez az időmérés a rendszerórát veszi figyelembe (wall clock time), és a számítások közben más folyamatok is ütemezésre kerülhettek, a pontos eredmény érdekében érdemes többször megmérni a futási időt, az kiugró értékeket eldobni, majd a többi mérésből átlagot számolni.\n",
        "\n",
        "A `clock_gettime` segítségével, POSIX kompatibilis rendszereken a `CLOCK_PROCESS_CPUTIME_ID` óra azonosító paraméter megadásával, folyamat szintű időmérést végezhetünk. A [https://en.cppreference.com/w/c/chrono/clock](https://en.cppreference.com/w/c/chrono/clock) található példa bemutatja a folyamat és rendszer idő közötti különbséget."
      ],
      "metadata": {
        "id": "9LtVX7QST6Sn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Véletlenszám generálás CUDA-ban\n",
        "\n",
        "A továbbiakban megismerkedünk a véletlen számok generálásának módszertanával CUDA-ban, a [`cuRAND`](https://docs.nvidia.com/cuda/curand/index.html)  könyvtár használatával.\n",
        "\n",
        "\n",
        "Egy álvéletlenszám sorozat generálása egy szekvenciális folyamat, ahol a következő számot az előbbi segítségével állítjuk elő. Például a [lineáris kongruenciális generátor](https://wikihuhu.top/wiki/Lehmer_random_number_generator) esetében\n",
        "\n",
        "\n",
        "```\n",
        "X_{n+1} = (aX_{n} + c) mod m\n",
        "```\n",
        "\n",
        "ahol ![](https://render.githubusercontent.com/render/math?math=$a$) a szorzó , ![](https://render.githubusercontent.com/render/math?math=$c$) a növekmény és ![](https://render.githubusercontent.com/render/math?math=$m$) a modulus.\n",
        "\n",
        "A kezdeti kifejezést magnak (angolul seed) nevezzük. Ez teszi lehetővé egy látszólag véletlenszerű szekvencia létrehozását. Minden maghoz új folytatást kapunk.\n",
        "\n",
        "Párhuzamosan generálni pszeudo-véletlen sorozattokat [kicsit bonyolultabb](https://quick-adviser.com/is-rand--thread-safe/\n",
        "), mivel egy naiv megközelítésben az `X_{n+1}` kiszámolásánál versenyhelyzet lép fel, több szál is ugyanazt  az értéket fogja generálni.\n",
        "\n",
        "\n",
        "Egy egyszerű megoldás, ha  minden szálhoz külön véletlenszerű állapotot hozunk létre, amelyet az egyes szálak a saját, a többiektől független véletlenszerű számsorozat létrehozásához használnak. Minden szál különböző magból, eredeti állapotból indul ki, annak érdekében, hogy mindegyikük egy különböző álváletlenszerű sorozatot generáljon.\n",
        "\n",
        "A [`cuRAND`](https://docs.nvidia.com/cuda/curand/index.html) könyvtár interfészt biztosít a véletlenszám-generátor állapotának inicializálásához szálanként, és ennek az állapotnak a felhasználásához véletlenszám-sorozatok generálásához. A következő lépések szükségesek ahhoz, hogy a cuRAND segítségével véletlenszerű számokat tudjunk generálni a CUDA programjainkban:\n",
        "\n",
        "1.\tA `cuRAND` fejlécek behívása.\n",
        "\n",
        "\t```cpp\n",
        "\t#include <curand_kernel.h>\n",
        "\t#include <curand.h>\n",
        "\t```\n",
        "2.\tMinden szálnak külön kell biztosítani egy eredeti magot,  különböző belső állapotot:\n",
        "\n",
        "\t-\tMemória lefoglalása a szálak állapváltozóinak (`curandState`).\n",
        "\n",
        "\t\t```cpp\n",
        "\t\tcurandState *dev_random;\n",
        "\t\tcudaMalloc((void**)&dev_random, num_threads_per_block*num_blocks*sizeof(curandState));\n",
        "\t\t```\n",
        "\n",
        "\t- Véletlenszerű állapotok inicializálása egy kernelben. Minden szál inicializálja a saját állapotát a GPU-n, a saját egyedi azonosítóját felhasználva:\n",
        "\n",
        "\t\t```cpp\n",
        "\t\t__global__ void gpu_random(..., curandState *states) {  \n",
        "\t\t\tint id = threadIdx.x + blockDim.x * blockIdx.x;  \n",
        "\n",
        "\t\t\t...  \n",
        "\t\t\tint seed = id; // different seed per thread         \n",
        "\t\t\tcurand_init(seed, id, 0, &states[id]);  //  Initialize CURAND        \n",
        "\t\t \t...\n",
        "\t\t ```\n",
        "\n",
        "3. Véletlen számok generálása a GPU-n valamilyen eloszlás szerint (pl. `curand_uniform`), szálanként a saját inicializált állapot használatával.\n",
        "\n",
        "\t```cpp\n",
        "\t__global__ void gpu_random(..., curandState *states) {  \n",
        "\t\tint id = threadIdx.x + blockDim.x * blockIdx.x;  float x;         \n",
        "\t\t...  \n",
        "\t\tcurand_init(seed, tid, 0, &states[id]);  //  Initialize CURAND         \n",
        "\t\t...\n",
        "\t\tfor(int i = 0; i < TRIALS_PER_THREAD; i++) {   \n",
        "\t\tx = curand_uniform (&states[id]);   \n",
        "\t\t...  \n",
        "\t}\n",
        "\t```\n",
        "\n",
        "\n",
        "A leggyakrabban használt eloszlások:\n",
        "\n",
        "```cpp\n",
        "// Egyenletes eloszlás\n",
        "__device__ float\n",
        "curand_uniform (curandState_t *state)\n",
        "\n",
        "// Normális eloszlás\n",
        "__device__ float\n",
        "curand_normal (curandState_t *state)\n",
        "\n",
        "// Log-normális eloszlás - a valószínűségi változó logaritmusa normális eloszlású.\n",
        "__device__ float\n",
        "curand_log_normal (curandState_t *state, float mean, float stddev)\n",
        "\n",
        "// Poisson-eloszlás\n",
        "__device__ unsigned int\n",
        "curand_poisson (curandState_t *state, double lambda)\n",
        "```\n",
        "\n",
        "Ezek a függvények elérhetőek dupla pontosságú lebegőpontos (`double`) számábrázolással is.\n",
        "\n",
        "## [Példa](https://github.com/deeperlearning/professional-cuda-c-programming/blob/master/examples/chapter08/rand-kernel.cu)\n",
        "\n",
        "```cpp\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "\n",
        "\n",
        "int threads_per_block = 256;\n",
        "int blocks_per_grid = 30;\n",
        "\n",
        "\n",
        "/*\n",
        " * device_api_kernel uses the cuRAND device API to generate random numbers\n",
        " * on-the-fly on the GPU, and then performs some dummy computation using them.\n",
        " */\n",
        "__global__ void device_api_kernel(curandState *states, float *out, int N)\n",
        "{\n",
        "    int i;\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int nthreads = gridDim.x * blockDim.x;\n",
        "    curandState *state = states + tid;\n",
        "\n",
        "    curand_init(9384, tid, 0, state);\n",
        "\n",
        "    for (i = tid; i < N; i += nthreads)\n",
        "    {\n",
        "        float rand = curand_uniform(state);\n",
        "        rand = rand * 2;\n",
        "        out[i] = rand;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "/*\n",
        " * use_device_api is an examples usage of the cuRAND device API to use the GPU\n",
        " * to generate random values on the fly from inside a CUDA kernel.\n",
        " */\n",
        "void use_device_api(int N)\n",
        "{\n",
        "    int i;\n",
        "    static curandState *states = NULL;\n",
        "    float *dOut, *hOut;\n",
        "\n",
        "    /*\n",
        "     * Allocate device memory to store the output and cuRAND device state\n",
        "     * objects (which are analogous to handles, but on the GPU).\n",
        "     */\n",
        "    cudaMalloc((void **)&dOut, sizeof(float) * N);\n",
        "    cudaMalloc((void **)&states, sizeof(curandState) *\n",
        "                threads_per_block * blocks_per_grid);\n",
        "    hOut = (float *)malloc(sizeof(float) * N);\n",
        "\n",
        "    // Execute a kernel that generates and consumes its own random numbers\n",
        "    device_api_kernel<<<blocks_per_grid, threads_per_block>>>(states, dOut, N);\n",
        "\n",
        "    // Retrieve the results\n",
        "    cudaMemcpy(hOut, dOut, sizeof(float) * N, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"Sampling of output from device API:\\n\");\n",
        "\n",
        "    for (i = 0; i < 10; i++)\n",
        "    {\n",
        "        printf(\"%2.4f\\n\", hOut[i]);\n",
        "    }\n",
        "\n",
        "    printf(\"...\\n\");\n",
        "\n",
        "    free(hOut);\n",
        "    cudaFree(dOut);\n",
        "    cudaFree(states);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "    int N = 8388608;\n",
        "\n",
        "    use_device_api(N);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zwNDgRyiALmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CUDA profilozása az NVIDIA Profiler segítségével\n",
        "\n",
        "A CUDA-programok hatékonyságának elemzéséhez gyakran nem elég csupán a kernel futási időt mérni, mivel ez nem ad részletes információt a **memóriahozzáférésekről, regiszterhasználatról, párhuzamos végrehajtás hatékonyságáról és az adatmozgatásokról**. Az [NVIDIA Profiler eszközei](https://developer.nvidia.com/performance-analysis-tools), például **nvprof, Nsight Systems és Nsight Compute**, lehetőséget adnak mélyebb teljesítményprofilozásra.  \n",
        "\n",
        "**Mivel más vagy több  az NVIDIA Profiler, mint egy egyszerű időmérés?**  \n",
        "- Megmutatja, hogy a kernel futásán belül hol vannak szűk keresztmetszetek.  \n",
        "- Elemzi a **memóriahasználatot** (globális, megosztott memória, L2 cache kihasználtság).  \n",
        "- Vizsgálja a **számítási kihasználtságot** (SM-kihasználtság, párhuzamos végrehajtás).  \n",
        "- Részletes jelentést nyújt az egyes CUDA API hívásokról és memóriamásolásokról.  \n",
        "\n",
        "Az [nvprof](https://docs.csc.fi/computing/nvprof/) eszköz egy parancssori profiler, nem igényel GUI-t, ezért Google Colab környezetben is könnyen tudjuk használni.\n",
        "\n",
        "\n",
        "Az nvprof-nak csak egyszerűen át kell adnunk a CUDA programunkat mint paraméter:\n",
        "\n",
        "```bash\n",
        "nvprof ./cuda_program\n",
        "```\n",
        "\n",
        "Ez majd automatikusan rögzíti az összes CUDA API hívást, kernel futtatást és memóriaátvitelt, majd egy összegző jelentést készít.\n",
        "\n",
        "---\n",
        "\n",
        "## Példa -  CUDA program profilozása\n",
        "Vizsgáljuk meg egy egyszerű CUDA-kernel futását és memóriahasználatát az nvprof segítségével.\n"
      ],
      "metadata": {
        "id": "gLqgmC6Hc6TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile profiletest.cu\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void vector_add(float *a, float *b, float *c, int n, unsigned long long seed) {\n",
        "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    // Véletlenszám-generátor inicializálása minden szálnál\n",
        "    curandState state;\n",
        "    curand_init(seed, i, 0, &state);\n",
        "\n",
        "    if (i < n) {\n",
        "        float noise = curand_uniform(&state) * 0.1f - 0.05f; // Zaj [-0.05, 0.05] intervallumban\n",
        "        c[i] = a[i] + b[i] + noise;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000000;\n",
        "    size_t size = n * sizeof(float);\n",
        "\n",
        "    // Memória foglalás a hoston\n",
        "    float *h_a = (float*)malloc(size);\n",
        "    float *h_b = (float*)malloc(size);\n",
        "    float *h_c = (float*)malloc(size);\n",
        "\n",
        "    // Inicializálás\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = 1.0f;\n",
        "        h_b[i] = 2.0f;\n",
        "    }\n",
        "\n",
        "    // Memória foglalás a GPU-n\n",
        "    float *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    // Adatok másolása a GPU-ra\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel indítása\n",
        "    int blockSize = 256;\n",
        "    int numBlocks = (n + blockSize - 1) / blockSize;\n",
        "    vector_add<<<numBlocks, blockSize>>>(d_a, d_b, d_c, n, 1234ULL);\n",
        "\n",
        "    // Eredmények visszamásolása\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Memória felszabadítás\n",
        "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
        "    free(h_a); free(h_b); free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jDQXyDIenbP",
        "outputId": "00f516fc-8b4f-4f45-da7d-8ad40b976873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting profiletest.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o vector_add_noise profiletest.cu"
      ],
      "metadata": {
        "id": "BvnpAwVGfw9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMb0LF4-gbVB",
        "outputId": "5bf4b75d-2996-41be-aa1f-0d238ef599a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvprof: NVIDIA (R) Cuda command line profiler\n",
            "Copyright (c) 2012 - 2024 NVIDIA Corporation\n",
            "Release version 12.5.82 (21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luX63OvbhCz_",
        "outputId": "972996b4-814c-4478-ae67-2c98794dfd9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 25 13:38:44 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Yd2C43etK7",
        "outputId": "717d409a-e9bb-445c-9ff4-11a587cc88e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2094== NVPROF is profiling process 2094, command: ./vector_add_noise\n",
            "==2094== Profiling application: ./vector_add_noise\n",
            "==2094== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   92.88%  40.913ms         1  40.913ms  40.913ms  40.913ms  vector_add(float*, float*, float*, int, __int64)\n",
            "                    3.91%  1.7237ms         1  1.7237ms  1.7237ms  1.7237ms  [CUDA memcpy DtoH]\n",
            "                    3.21%  1.4135ms         2  706.77us  694.38us  719.15us  [CUDA memcpy HtoD]\n",
            "      API calls:   69.89%  108.99ms         3  36.330ms  77.995us  108.83ms  cudaMalloc\n",
            "                   29.34%  45.762ms         3  15.254ms  897.51us  43.957ms  cudaMemcpy\n",
            "                    0.36%  566.31us         3  188.77us  151.47us  208.06us  cudaFree\n",
            "                    0.30%  469.98us         1  469.98us  469.98us  469.98us  cudaLaunchKernel\n",
            "                    0.09%  143.06us       114  1.2540us     103ns  57.994us  cuDeviceGetAttribute\n",
            "                    0.01%  11.399us         1  11.399us  11.399us  11.399us  cuDeviceGetName\n",
            "                    0.00%  6.0250us         1  6.0250us  6.0250us  6.0250us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.7360us         3     578ns     151ns  1.2900us  cuDeviceGetCount\n",
            "                    0.00%     846ns         2     423ns     114ns     732ns  cuDeviceGet\n",
            "                    0.00%     468ns         1     468ns     468ns     468ns  cuModuleGetLoadingMode\n",
            "                    0.00%     352ns         1     352ns     352ns     352ns  cuDeviceTotalMem\n",
            "                    0.00%     277ns         1     277ns     277ns     277ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mit tudunk meg ebből?**\n",
        "- A **vector_add kernel** a teljes futási idő **92.15%-át** teszi ki.\n",
        "- Az **adatmásolás** (Host → Device és Device → Host) mennyi időt vesz igénybe (~4.47% + 3.38%).\n",
        "- Milyen API hívások hajtodtak végre.\n",
        "\n",
        "\n",
        "## Részletesebb Elemzés\n",
        "Az nvprof további hasznos lehetőségeket biztosít. A GPU-műveletek részletes megjelenítése hasnzáljuk:\n"
      ],
      "metadata": {
        "id": "OeAUXUuVetmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZliB41lQjPTr",
        "outputId": "3f039550-31ef-4602-ce34-69ea94d1e0bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2823== NVPROF is profiling process 2823, command: ./vector_add_noise\n",
            "==2823== Profiling application: ./vector_add_noise\n",
            "==2823== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "243.29ms  696.81us                    -               -         -         -         -  3.8147MB  5.3462GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "244.22ms  707.98us                    -               -         -         -         -  3.8147MB  5.2619GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "245.41ms  40.934ms           (3907 1 1)       (256 1 1)        61        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  vector_add(float*, float*, float*, int, __int64) [130]\n",
            "286.35ms  1.6342ms                    -               -         -         -         -  3.8147MB  2.2796GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "nvprof --print-gpu-trace ./vector_add\n",
        "```\n",
        "\n",
        "A profilozás az alkalmazás futtatásának különböző aspektusait méri, mint például a memória másolásokat, kernel futtatásokat, és a GPU erőforrások használatát. A profilozás minden CUDA művelet pontos időbélyegét megmutatja. Pontosan mit láthatunk?\n",
        "\n",
        "\n",
        "### Értelmezés\n",
        "\n",
        "1. CUDA Memcpy HtoD (Host-to-Device Memory Copy)\n",
        "- **`243.29ms`** és **`244.22ms`**: Ez az időpont, amikor a memória másolás elkezdődik és befejeződik. A másolás a host (CPU) és a device (GPU) között történik.\n",
        "- **`696.81us` és `707.98us`**: A memória másolás (Host to Device) ideje, tehát mennyi időbe telt az adatokat átmásolni a CPU-ról a GPU-ra.\n",
        "- **`3.8147MB`**: A memória másolás mérete. Ez azt jelenti, hogy az adatok mérete körülbelül 3.8 MB volt, amit átvittünk.\n",
        "- **`5.3462GB/s` és `5.2619GB/s`**: A memória másolás sebessége, azaz hány GB adatot másoltunk másodpercenként.\n",
        "\n",
        "A **`SrcMemType`** és **`DstMemType`** az adatok forrás- és célmemóriájának típusát jelzi.\n",
        "- **`Pageable`**: Ez azt jelenti, hogy a memóriát nem tartja fixen a rendszer, így a memória elérhetősége rugalmas.\n",
        "- **`Device`**: A célmemória a GPU memóriája.\n",
        "\n",
        "\n",
        "2. **CUDA Memcpy DtoH (Device-to-Host Memory Copy)**\n",
        "- **`286.35ms`** és **`1.6342ms`**: A másolás befejeződése és az időtartam. Ez az adat visszamásolását jelenti a GPU-ról a CPU-ra.\n",
        "- **`3.8147MB`**: A visszamásolt adat mennyisége.\n",
        "- **`2.2796GB/s`**: Az adatok átvitelének sebessége a GPU-ról a CPU-ra.\n",
        "\n",
        "\n",
        "3. **Kernel futás: `vector_add`**\n",
        "- **`245.41ms`**: A kernel végrehajtásának időtartama, ami a `vector_add` nevű függvény futtatása.\n",
        "- **`40.934ms`**: Ez az idő, amely alatt a kernel ténylegesen végrehajtódott.\n",
        "- **`(3907 1 1)`**: A **grid mérete** (háromdimenziós), ami azt jelzi, hogy hány blokkot futtatott a kernel: 3907 blokkot.\n",
        "- **`(256 1 1)`**: A **block mérete** (háromdimenziós), ami azt jelzi, hogy hány szálat futtatott egy blokkban: 256 szál.\n",
        "- **`61`**: A kernel által használt **regiszterek** száma szálanként. A regiszterek a CPU/GPU gyors memóriája, amely a szálak gyors adatkezeléséhez szükségesek.\n",
        "- **`0B`**: **Static shared memory (SSMem)** és **Dynamic shared memory (DSMem)**: Ez a memória, amelyet a blokk szálai osztanak meg egymással. Most nem lett használva sem statikus, sem dinamikus memória.\n",
        "  \n",
        "- A kernel neve: **`vector_add(float*, float*, float*, int, __int64)`**, ami azt jelzi, hogy egy vektorműveletet hajtott végre, ahol három lebegőpontos számokból álló vektort adtak össze.\n",
        "\n",
        "\n",
        "\n",
        "### Következtetés\n",
        "\n",
        "- Az adatátvitel (Host-to-Device és Device-to-Host) viszonylag gyors volt, 5-5 GB/s sebességgel.\n",
        "- A **`vector_add`** kernel futása 245.41ms-ig tartott, és az átlagos végrehajtási idő 40.934ms volt. Az adatokat 3907 blokkban és 256 szálat használva dolgozták fel.\n",
        "- A regiszterek száma szálanként 61, ami azt jelzi, hogy nem használtak túl sok regisztert a kernelben. A kernel nem használt megosztott memóriát (sem statikus, sem dinamikus).\n",
        "\n",
        "Mint látjuk, a profilozás segít abban, hogy áttekinthessük a program GPU erőforrás-használatát, és láthatjuk, hogy a memória másolás és a kernel végrehajtás hogyan befolyásolja a program teljesítményét.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OgczpNuQjPi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memóriahasználat Profilozása"
      ],
      "metadata": {
        "id": "rFQAeDfIk--F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-summary ./vector_add_noise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VYub3JOk_Q_",
        "outputId": "2b67dd1e-a7b3-4d4d-c97f-96d6900ad098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==4856== NVPROF is profiling process 4856, command: ./vector_add_noise\n",
            "==4856== Profiling application: ./vector_add_noise\n",
            "==4856== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   92.87%  40.918ms         1  40.918ms  40.918ms  40.918ms  vector_add(float*, float*, float*, int, __int64)\n",
            "                    3.68%  1.6196ms         1  1.6196ms  1.6196ms  1.6196ms  [CUDA memcpy DtoH]\n",
            "                    3.45%  1.5220ms         2  761.00us  743.66us  778.35us  [CUDA memcpy HtoD]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Az **`nvprof --print-gpu-summary`** parancs összegző profilozási adatokat adott vissza a CUDA programról, beleértve a GPU aktivitások időarányos megoszlását, valamint a különböző kernel-ek és memória műveletek végrehajtásának időtartamát.\n",
        "\n",
        "A következő adatokat láthatjuk az eredményben:\n",
        "\n",
        "1. GPU aktivitások:\n",
        "- **`92.87%`**: A **GPU aktivitások összes idejének 92.87%-át** a `vector_add` kernel végrehajtása tette ki. Ez azt jelzi, hogy a program fő részét a kernel futtatása képezte.\n",
        "  - **`40.918ms`**: A `vector_add` kernel teljes futási ideje.\n",
        "  - **`1`**: A kernel pontosan **1 alkalommal** futott.\n",
        "  - **`40.918ms`**: Az egyes futások időtartama, ami azonos minden egyes kernel végrehajtás esetén (mivel csak egyszer futott).\n",
        "\n",
        "2. **[CUDA memcpy DtoH] (Device-to-Host Memory Copy):\n",
        "- **`3.68%`**: A memória másolás a GPU-ról a CPU-ra (Device-to-Host) **3.68%-ban** tette ki a teljes időt.\n",
        "  - **`1.6196ms`**: Ez a másolás teljes időtartama.\n",
        "  - **`1`**: A memória másolás pontosan **1 alkalommal** történt.\n",
        "  - **`1.6196ms`**: Az egyes memória másolások időtartama (mivel csak egyszer történt másolás).\n",
        "\n",
        "3. **[CUDA memcpy HtoD] (Host-to-Device Memory Copy):\n",
        "- **`3.45%`**: A memória másolás a CPU-ról a GPU-ra (Host-to-Device) **3.45%-ban** tette ki a teljes időt.\n",
        "  - **`1.5220ms`**: A memória másolás teljes időtartama (összesen két másolás történt).\n",
        "  - **`2`**: A memória másolás **2 alkalommal** történt.\n",
        "  - **`761.00us`**: Az egyes másolások átlagos időtartama (768us és 743us között ingadozott).\n",
        "  - **`743.66us`** és **`778.35us`**: A másolások közötti időeltérés, tehát az időintervallumok minimális és maximális értékei.\n",
        "\n",
        "Összegzés:\n",
        "- **A `vector_add` kernel** a legnagyobb részét tette ki a program futásának, és **40.918 ms**-ot igényelt.\n",
        "- **Memória másolás**:\n",
        "  - A **GPU-ról a CPU-ra történő másolás** (`[CUDA memcpy DtoH]`) körülbelül **1.6196 ms**-ot vett igénybe.\n",
        "  - A **CPU-ról a GPU-ra történő másolás** (`[CUDA memcpy HtoD]`) összesen **1.5220 ms** időt vett igénybe, amelyet két különböző másolás hajtott végre.\n",
        "\n",
        "Itt is látjuk, a legnagyobb időt a **kernel végrehajtása** tette ki, míg a memória másolás (GPU és CPU között) viszonylag kis időt vett igénybe az összes futási időhöz képest.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Általános optimalizálási tippek\n",
        "- Memóriahasználat csökkentése: Ha a memória másolás időigényes, próbáljunk meg kevesebb adatot mozgatni.  \n",
        "- Regiszterhasználat figyelése: Ha túl sok a regiszter, kevesebb thread tud futni párhuzamosan.  \n",
        "- Optimalizált grid/block méretek: Ha a kihasználtság alacsony, próbáljunk meg más **blockSize** értékeket.  \n",
        "- Ha az L2 cache hit rate alacsony, érdemes **megosztott memóriát (shared memory)** használni a teljesítmény növelése érdekében.\n",
        "\n",
        "\n",
        "## nvprof összegzés\n",
        "| **Parancs** | **Mit csinál?** |\n",
        "|-------------|----------------|\n",
        "| `nvprof ./program` | Alapvető profilozás, futási idők elemzése |\n",
        "| `nvprof --print-gpu-trace ./program` | Részletes GPU műveletek megjelenítése |\n",
        "| `nvprof --print-gpu-summary ./program` | Memóriahasználat és kihasználtság elemzése |\n",
        "\n",
        "Az nvprof egy egyszerű, de erőteljes parancssori eszköz a CUDA-programok teljesítményének mérésére. Ha mélyebb elemzést szeretnénk, az **Nsight Systems és Nsight Compute** további részleteket adhat.\n"
      ],
      "metadata": {
        "id": "duSCPZCmk_cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feladatok\n",
        "\n",
        "1. Írjunk egy CUDA programot, amely véletlenszerű pontokat generál az egységnégyzetben \\([0,1] \\times [0,1]\\), majd kiszámolja a pontok távolságát az origótól.  \n",
        "  Lépések:\n",
        "  - Minden CUDA szál generáljon egy véletlen \\( (x, y) \\) pontot a \\([0,1]\\) tartományban.  \n",
        "  - Számoljuk ki a pontok távolságát az origótól:  \n",
        "   $d = \\sqrt{x^2 + y^2}$\n",
        "  - Tároljuk az eredményeket egy tömbben.  \n",
        "  - Másoljuk vissza az adatokat a CPU-ra, és számoljuk ki az átlagos távolságot.  \n",
        "\n",
        "  Az átlagos távolságnak közel kell lennie az elméleti értékhez:  $E[d] \\approx 0.521405$\n",
        "\n",
        "2. Írjunk egy szekvenciális majd egy CUDA programot mely Monte Carlo módszer segítségével, ![](https://render.githubusercontent.com/render/math?math=$n$) véletlenszerű pontot generálva, megközelítően kiszámítja a ![](https://render.githubusercontent.com/render/math?math=$\\pi$) értékét. Az ![](https://render.githubusercontent.com/render/math?math=$n$) a program paramétere.\n",
        "\n",
        "  A módszer leírása:\n",
        "    \n",
        "  - [http://nagysandor.eu/physlet/applets/iter1.html](http://nagysandor.eu/physlet/applets/iter1.html)\n",
        "\n",
        "  - [http://www.tldp.org/HOWTO/Parallel-Processing-HOWTO-2.html](http://www.tldp.org/HOWTO/Parallel-Processing-HOWTO-2.html)\n",
        "\n",
        "3. A tanult CUDA időmérést felhasználva számoljuk ki CUDA-programjaink gyorsulását. Ehhez természetesen szükség van a szekvenciális verziókra is, amelyek a hoston futnak.  \n",
        "\n",
        "  A hoston az időmérést a `chrono::high_resolution_clock` segítségével végezzük, míg a CUDA-kernélek esetében a `cudaEventCreate` és `cudaEventRecord` függvényeket használjuk.\n",
        "\n",
        "4. Profilozzuk a megírt programokat. Vizsgáljuk meg a kernelek futási idejét, a regiszterek és memóriahasználatot, majd az észrevételeket egy rövid beszámolóban rögzítsük (ez lehet külön fájl vagy egy új \"text\" cella a notebook-ban).\n"
      ],
      "metadata": {
        "id": "OHJTyfOOHyjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile elsofeladat.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h>\n",
        "#include <chrono>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void generate_points_and_distances(float *distances, int n, unsigned long long seed) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    if (idx < n) {\n",
        "        curandState state;\n",
        "        curand_init(seed, idx, 0, &state);\n",
        "\n",
        "        float x = curand_uniform(&state);\n",
        "        float y = curand_uniform(&state);\n",
        "\n",
        "        float distance = sqrtf(x * x + y * y);\n",
        "\n",
        "        distances[idx] = distance;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 1000000;  // 1 millió pont\n",
        "    const int blockSize = 256;\n",
        "    const int numBlocks = (n + blockSize - 1) / blockSize;\n",
        "    const size_t size = n * sizeof(float);\n",
        "\n",
        "    printf(\"CUDA Véletlenszerű Pontok Távolságszámítás\\n\");\n",
        "    printf(\"=========================================\\n\");\n",
        "    printf(\"Pontok száma: %d\\n\", n);\n",
        "    printf(\"Block méret: %d\\n\", blockSize);\n",
        "    printf(\"Blokkok száma: %d\\n\", numBlocks);\n",
        "    printf(\"Elméleti átlagos távolság: ~0.521405\\n\\n\");\n",
        "\n",
        "    float *h_distances = (float*)malloc(size);\n",
        "    if (h_distances == NULL) {\n",
        "        printf(\"Hiba: Host memória foglalás sikertelen!\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    float *d_distances;\n",
        "    cudaError_t cudaStatus = cudaMalloc(&d_distances, size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Device memória foglalás sikertelen!\\n\");\n",
        "        free(h_distances);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    auto cpu_start = chrono::high_resolution_clock::now();\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    generate_points_and_distances<<<numBlocks, blockSize>>>(d_distances, n, 12345ULL);\n",
        "\n",
        "    cudaStatus = cudaDeviceSynchronize();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Kernel végrehajtás sikertelen! %s\\n\", cudaGetErrorString(cudaStatus));\n",
        "        cudaFree(d_distances);\n",
        "        free(h_distances);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaStatus = cudaMemcpy(h_distances, d_distances, size, cudaMemcpyDeviceToHost);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Memória másolás sikertelen!\\n\");\n",
        "        cudaFree(d_distances);\n",
        "        free(h_distances);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    auto cpu_end = chrono::high_resolution_clock::now();\n",
        "\n",
        "    double sum = 0.0;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        sum += h_distances[i];\n",
        "    }\n",
        "    double average_distance = sum / n;\n",
        "\n",
        "    float cuda_time;\n",
        "    cudaEventElapsedTime(&cuda_time, start, stop);\n",
        "\n",
        "    auto cpu_duration = chrono::duration_cast<chrono::milliseconds>(cpu_end - cpu_start);\n",
        "\n",
        "    printf(\"Eredmények:\\n\");\n",
        "    printf(\"-----------\\n\");\n",
        "    printf(\"Számított átlagos távolság: %.6f\\n\", average_distance);\n",
        "    printf(\"Eltérés az elméleti értéktől: %.6f\\n\", fabs(average_distance - 0.521405));\n",
        "    printf(\"\\nTeljesítmény:\\n\");\n",
        "    printf(\"CUDA kernel futási idő: %.2f ms\\n\", cuda_time);\n",
        "    printf(\"Teljes CPU idő: %ld ms\\n\", cpu_duration.count());\n",
        "    printf(\"Feldolgozott pontok/másodperc: %.0f\\n\", n / (cuda_time / 1000.0));\n",
        "\n",
        "    float min_dist = h_distances[0];\n",
        "    float max_dist = h_distances[0];\n",
        "    for (int i = 1; i < n; i++) {\n",
        "        if (h_distances[i] < min_dist) min_dist = h_distances[i];\n",
        "        if (h_distances[i] > max_dist) max_dist = h_distances[i];\n",
        "    }\n",
        "\n",
        "    printf(\"\\nStatisztikák:\\n\");\n",
        "    printf(\"Minimális távolság: %.6f\\n\", min_dist);\n",
        "    printf(\"Maximális távolság: %.6f\\n\", max_dist);\n",
        "    printf(\"Tartomány: [%.6f, %.6f]\\n\", min_dist, max_dist);\n",
        "\n",
        "    printf(\"\\nMintaértékek (első 10 távolság):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%.6f \", h_distances[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "\n",
        "    cudaStatus = cudaGetLastError();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"CUDA hiba: %s\\n\", cudaGetErrorString(cudaStatus));\n",
        "    }\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaFree(d_distances);\n",
        "    free(h_distances);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "PJLNtmjbn2N5",
        "outputId": "61ef1a22-5a51-4464-9964-b6b7c30bd680",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting elsofeladat.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o elsofeladat elsofeladat.cu"
      ],
      "metadata": {
        "id": "YEZV1gbMwkwI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./elsofeladat"
      ],
      "metadata": {
        "id": "GrV8fF0Bw-JM",
        "outputId": "9bbf0597-56ca-4491-bcb1-1fd027b2dfac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Véletlenszerű Pontok Távolságszámítás\n",
            "=========================================\n",
            "Pontok száma: 1000000\n",
            "Block méret: 256\n",
            "Blokkok száma: 3907\n",
            "Elméleti átlagos távolság: ~0.521405\n",
            "\n",
            "Eredmények:\n",
            "-----------\n",
            "Számított átlagos távolság: 0.765454\n",
            "Eltérés az elméleti értéktől: 0.244049\n",
            "\n",
            "Teljesítmény:\n",
            "CUDA kernel futási idő: 41.36 ms\n",
            "Teljes CPU idő: 44 ms\n",
            "Feldolgozott pontok/másodperc: 24175593\n",
            "\n",
            "Statisztikák:\n",
            "Minimális távolság: 0.002267\n",
            "Maximális távolság: 1.413326\n",
            "Tartomány: [0.002267, 1.413326]\n",
            "\n",
            "Mintaértékek (első 10 távolság):\n",
            "0.484252 0.864508 0.719313 1.323977 0.083039 0.411939 1.020200 0.805797 0.667082 0.453043 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile masodikfeladat.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "#include <curand_kernel.h>\n",
        "#include <math.h>\n",
        "#include <chrono>\n",
        "#include <random>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void monte_carlo_pi_kernel(unsigned long long *hits_inside, unsigned long long n_per_thread, unsigned long long seed) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    curandState state;\n",
        "    curand_init(seed, idx, 0, &state);\n",
        "\n",
        "    unsigned long long local_hits = 0;\n",
        "\n",
        "    for (unsigned long long i = 0; i < n_per_thread; i++) {\n",
        "        float x = curand_uniform(&state) * 2.0f - 1.0f;  // [-1, 1]\n",
        "        float y = curand_uniform(&state) * 2.0f - 1.0f;  // [-1, 1]\n",
        "\n",
        "        if (x * x + y * y <= 1.0f) {\n",
        "            local_hits++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    hits_inside[idx] = local_hits;\n",
        "}\n",
        "\n",
        "double sequential_monte_carlo_pi(unsigned long long n) {\n",
        "\n",
        "    auto start_time = chrono::high_resolution_clock::now();\n",
        "\n",
        "    random_device rd;\n",
        "    mt19937 gen(rd());\n",
        "    uniform_real_distribution<double> dis(-1.0, 1.0);\n",
        "\n",
        "    unsigned long long hits_inside = 0;\n",
        "\n",
        "    for (unsigned long long i = 0; i < n; i++) {\n",
        "        double x = dis(gen);\n",
        "        double y = dis(gen);\n",
        "\n",
        "        if (x * x + y * y <= 1.0) {\n",
        "            hits_inside++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    auto end_time = chrono::high_resolution_clock::now();\n",
        "    auto duration = chrono::duration_cast<chrono::milliseconds>(end_time - start_time);\n",
        "\n",
        "    double pi_estimate = 4.0 * hits_inside / n;\n",
        "\n",
        "    printf(\"Szekvenciális eredmények:\\n\");\n",
        "    printf(\"Pontok száma: %llu\\n\", n);\n",
        "    printf(\"Körön belüli pontok: %llu\\n\", hits_inside);\n",
        "    printf(\"π becslés: %.10f\\n\", pi_estimate);\n",
        "    printf(\"Eltérés a valós π-től: %.10f\\n\", fabs(pi_estimate - M_PI));\n",
        "    printf(\"Futási idő: %ld ms\\n\", duration.count());\n",
        "    printf(\"Pontok/másodperc: %.0f\\n\\n\", n / (duration.count() / 1000.0));\n",
        "\n",
        "    return pi_estimate;\n",
        "}\n",
        "\n",
        "double cuda_monte_carlo_pi(unsigned long long n, int blockSize) {\n",
        "\n",
        "    int numBlocks = min(65535, (int)((n + blockSize - 1) / blockSize));\n",
        "    unsigned long long points_per_thread = (n + numBlocks * blockSize - 1) / (numBlocks * blockSize);\n",
        "    unsigned long long total_points = numBlocks * blockSize * points_per_thread;\n",
        "\n",
        "    printf(\"Block méret: %d\\n\", blockSize);\n",
        "    printf(\"Blokkok száma: %d\\n\", numBlocks);\n",
        "    printf(\"Pontok szálanként: %llu\\n\", points_per_thread);\n",
        "    printf(\"Összes feldolgozott pont: %llu\\n\", total_points);\n",
        "\n",
        "    unsigned long long *d_hits;\n",
        "    size_t hits_size = numBlocks * blockSize * sizeof(unsigned long long);\n",
        "    cudaError_t cudaStatus = cudaMalloc(&d_hits, hits_size);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Device memória foglalás sikertelen!\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    monte_carlo_pi_kernel<<<numBlocks, blockSize>>>(d_hits, points_per_thread, 42ULL);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaStatus = cudaDeviceSynchronize();\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Kernel végrehajtás sikertelen! %s\\n\", cudaGetErrorString(cudaStatus));\n",
        "        cudaFree(d_hits);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    unsigned long long *h_hits = (unsigned long long*)malloc(hits_size);\n",
        "    cudaStatus = cudaMemcpy(h_hits, d_hits, hits_size, cudaMemcpyDeviceToHost);\n",
        "    if (cudaStatus != cudaSuccess) {\n",
        "        printf(\"Hiba: Memória másolás sikertelen!\\n\");\n",
        "        cudaFree(d_hits);\n",
        "        free(h_hits);\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    unsigned long long total_hits = 0;\n",
        "    for (int i = 0; i < numBlocks * blockSize; i++) {\n",
        "        total_hits += h_hits[i];\n",
        "    }\n",
        "\n",
        "    float cuda_time;\n",
        "    cudaEventElapsedTime(&cuda_time, start, stop);\n",
        "\n",
        "    double pi_estimate = 4.0 * total_hits / total_points;\n",
        "\n",
        "    printf(\"CUDA eredmények:\\n\");\n",
        "    printf(\"Feldolgozott pontok: %llu\\n\", total_points);\n",
        "    printf(\"Körön belüli pontok: %llu\\n\", total_hits);\n",
        "    printf(\"π becslés: %.10f\\n\", pi_estimate);\n",
        "    printf(\"Eltérés a valós π-től: %.10f\\n\", fabs(pi_estimate - M_PI));\n",
        "    printf(\"CUDA kernel futási idő: %.2f ms\\n\", cuda_time);\n",
        "    printf(\"Pontok/másodperc: %.0f\\n\\n\", total_points / (cuda_time / 1000.0));\n",
        "\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "    cudaFree(d_hits);\n",
        "    free(h_hits);\n",
        "\n",
        "    return pi_estimate;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    printf(\"Monte Carlo π Számítás - Szekvenciális vs CUDA\\n\");\n",
        "    printf(\"===============================================\\n\");\n",
        "    printf(\"Valós π érték: %.10f\\n\\n\", M_PI);\n",
        "\n",
        "    unsigned long long n = 10000000;\n",
        "    int blockSize = 256;\n",
        "\n",
        "    if (argc >= 2) {\n",
        "        n = strtoull(argv[1], NULL, 10);\n",
        "        if (n == 0) {\n",
        "            printf(\"Hiba: Érvénytelen pontszám!\\n\");\n",
        "            printf(\"Használat: %s <pontok_száma> [block_méret]\\n\", argv[0]);\n",
        "            return -1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (argc >= 3) {\n",
        "        blockSize = atoi(argv[2]);\n",
        "        if (blockSize <= 0 || blockSize > 1024) {\n",
        "            printf(\"Hiba: Érvénytelen block méret! (1-1024 között)\\n\");\n",
        "            return -1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    printf(\"Paraméterek:\\n\");\n",
        "    printf(\"Pontok száma: %llu\\n\", n);\n",
        "    printf(\"CUDA block méret: %d\\n\\n\", blockSize);\n",
        "\n",
        "    auto seq_start = chrono::high_resolution_clock::now();\n",
        "    double pi_sequential = sequential_monte_carlo_pi(n);\n",
        "    auto seq_end = chrono::high_resolution_clock::now();\n",
        "    auto seq_duration = chrono::duration_cast<chrono::milliseconds>(seq_end - seq_start);\n",
        "\n",
        "    auto cuda_start = chrono::high_resolution_clock::now();\n",
        "    double pi_cuda = cuda_monte_carlo_pi(n, blockSize);\n",
        "    auto cuda_end = chrono::high_resolution_clock::now();\n",
        "    auto cuda_duration = chrono::duration_cast<chrono::milliseconds>(cuda_end - cuda_start);\n",
        "\n",
        "    printf(\"Összehasonlítás:\\n\");\n",
        "    printf(\"================\\n\");\n",
        "    printf(\"Szekvenciális π: %.10f (eltérés: %.10f)\\n\", pi_sequential, fabs(pi_sequential - M_PI));\n",
        "    printf(\"CUDA π:          %.10f (eltérés: %.10f)\\n\", pi_cuda, fabs(pi_cuda - M_PI));\n",
        "    printf(\"Különbség:       %.10f\\n\\n\", fabs(pi_sequential - pi_cuda));\n",
        "\n",
        "    printf(\"Teljesítmény:\\n\");\n",
        "    printf(\"=============\\n\");\n",
        "    printf(\"Szekvenciális teljes idő: %ld ms\\n\", seq_duration.count());\n",
        "    printf(\"CUDA teljes idő:          %ld ms\\n\", cuda_duration.count());\n",
        "\n",
        "    if (seq_duration.count() > 0 && cuda_duration.count() > 0) {\n",
        "        double speedup = (double)seq_duration.count() / cuda_duration.count();\n",
        "        printf(\"Gyorsítás:                %.2fx\\n\", speedup);\n",
        "\n",
        "        printf(\"Elméleti hatékonyság:     %.2f%%\\n\", (speedup / 1000.0) * 100); // Példa 1000 core GPU-ra\n",
        "    }\n",
        "\n",
        "    printf(\"\\nPrecizitás elemzés:\\n\");\n",
        "    printf(\"==================\\n\");\n",
        "    double theoretical_error = 1.96 / sqrt(n);  // 95% konfidencia intervallum\n",
        "    printf(\"Elméleti hiba (95%% CI):   ±%.6f\\n\", theoretical_error);\n",
        "    printf(\"Szekvenciális hiba:       %.6f\\n\", fabs(pi_sequential - M_PI));\n",
        "    printf(\"CUDA hiba:                %.6f\\n\", fabs(pi_cuda - M_PI));\n",
        "\n",
        "    if (fabs(pi_sequential - M_PI) > theoretical_error * 2) {\n",
        "        printf(\"- Növelje a mintavételezés számát a jobb pontosságért\\n\");\n",
        "    }\n",
        "    if (seq_duration.count() < cuda_duration.count()) {\n",
        "        printf(\"- Kis mintaszámnál a CUDA overhead nagyobb lehet\\n\");\n",
        "        printf(\"- Próbáljon nagyobb pontszámot (pl. %llu)\\n\", n * 10);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "d7tGlzkh0hWy",
        "outputId": "d0288e1e-11a4-4ee3-921c-501255601774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting masodikfeladat.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 -o masodikfeladat masodikfeladat.cu"
      ],
      "metadata": {
        "id": "f4S_57W50wR1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./masodikfeladat"
      ],
      "metadata": {
        "id": "LS0Tdl5g02r9",
        "outputId": "d2284326-174a-4133-aa04-61e6105eceb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo π Számítás - Szekvenciális vs CUDA\n",
            "===============================================\n",
            "Valós π érték: 3.1415926536\n",
            "\n",
            "Paraméterek:\n",
            "Pontok száma: 10000000\n",
            "CUDA block méret: 256\n",
            "\n",
            "Szekvenciális eredmények:\n",
            "Pontok száma: 10000000\n",
            "Körön belüli pontok: 7853511\n",
            "π becslés: 3.1414044000\n",
            "Eltérés a valós π-től: 0.0001882536\n",
            "Futási idő: 1373 ms\n",
            "Pontok/másodperc: 7283321\n",
            "\n",
            "Block méret: 256\n",
            "Blokkok száma: 39063\n",
            "Pontok szálanként: 1\n",
            "Összes feldolgozott pont: 10000128\n",
            "CUDA eredmények:\n",
            "Feldolgozott pontok: 10000128\n",
            "Körön belüli pontok: 7851674\n",
            "π becslés: 3.1406293999\n",
            "Eltérés a valós π-től: 0.0009632536\n",
            "CUDA kernel futási idő: 321.59 ms\n",
            "Pontok/másodperc: 31096167\n",
            "\n",
            "Összehasonlítás:\n",
            "================\n",
            "Szekvenciális π: 3.1414044000 (eltérés: 0.0001882536)\n",
            "CUDA π:          3.1406293999 (eltérés: 0.0009632536)\n",
            "Különbség:       0.0007750001\n",
            "\n",
            "Teljesítmény:\n",
            "=============\n",
            "Szekvenciális teljes idő: 1373 ms\n",
            "CUDA teljes idő:          601 ms\n",
            "Gyorsítás:                2.28x\n",
            "Elméleti hatékonyság:     0.23%\n",
            "\n",
            "Precizitás elemzés:\n",
            "==================\n",
            "Elméleti hiba (95% CI):   ±0.000620\n",
            "Szekvenciális hiba:       0.000188\n",
            "CUDA hiba:                0.000963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./elsofeladat"
      ],
      "metadata": {
        "id": "fTpydCjs42N8",
        "outputId": "05582063-3616-4f9c-aeb1-5e38f79de7b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Véletlenszerű Pontok Távolságszámítás\n",
            "=========================================\n",
            "Pontok száma: 1000000\n",
            "Block méret: 256\n",
            "Blokkok száma: 3907\n",
            "Elméleti átlagos távolság: ~0.521405\n",
            "\n",
            "==10168== NVPROF is profiling process 10168, command: ./elsofeladat\n",
            "Eredmények:\n",
            "-----------\n",
            "Számított átlagos távolság: 0.765454\n",
            "Eltérés az elméleti értéktől: 0.244049\n",
            "\n",
            "Teljesítmény:\n",
            "CUDA kernel futási idő: 41.43 ms\n",
            "Teljes CPU idő: 44 ms\n",
            "Feldolgozott pontok/másodperc: 24138525\n",
            "\n",
            "Statisztikák:\n",
            "Minimális távolság: 0.002267\n",
            "Maximális távolság: 1.413326\n",
            "Tartomány: [0.002267, 1.413326]\n",
            "\n",
            "Mintaértékek (első 10 távolság):\n",
            "0.484252 0.864508 0.719313 1.323977 0.083039 0.411939 1.020200 0.805797 0.667082 0.453043 \n",
            "==10168== Profiling application: ./elsofeladat\n",
            "==10168== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   96.24%  40.872ms         1  40.872ms  40.872ms  40.872ms  generate_points_and_distances(float*, int, __int64)\n",
            "                    3.76%  1.5968ms         1  1.5968ms  1.5968ms  1.5968ms  [CUDA memcpy DtoH]\n",
            "      API calls:   80.25%  182.05ms         1  182.05ms  182.05ms  182.05ms  cudaMalloc\n",
            "                   18.02%  40.875ms         1  40.875ms  40.875ms  40.875ms  cudaDeviceSynchronize\n",
            "                    1.31%  2.9621ms         1  2.9621ms  2.9621ms  2.9621ms  cudaMemcpy\n",
            "                    0.24%  534.09us         1  534.09us  534.09us  534.09us  cudaLaunchKernel\n",
            "                    0.08%  173.72us         1  173.72us  173.72us  173.72us  cudaFree\n",
            "                    0.07%  167.44us       114  1.4680us     103ns  73.442us  cuDeviceGetAttribute\n",
            "                    0.01%  25.955us         2  12.977us  12.330us  13.625us  cudaEventRecord\n",
            "                    0.01%  14.556us         1  14.556us  14.556us  14.556us  cuDeviceGetName\n",
            "                    0.01%  13.119us         2  6.5590us     675ns  12.444us  cudaEventCreate\n",
            "                    0.00%  7.4620us         1  7.4620us  7.4620us  7.4620us  cudaEventElapsedTime\n",
            "                    0.00%  5.6820us         1  5.6820us  5.6820us  5.6820us  cuDeviceGetPCIBusId\n",
            "                    0.00%  5.6650us         2  2.8320us     993ns  4.6720us  cudaEventDestroy\n",
            "                    0.00%  4.3050us         1  4.3050us  4.3050us  4.3050us  cudaEventSynchronize\n",
            "                    0.00%  1.8010us         3     600ns     112ns  1.0080us  cuDeviceGetCount\n",
            "                    0.00%  1.7780us         1  1.7780us  1.7780us  1.7780us  cudaGetLastError\n",
            "                    0.00%     823ns         2     411ns     225ns     598ns  cuDeviceGet\n",
            "                    0.00%     777ns         1     777ns     777ns     777ns  cuDeviceTotalMem\n",
            "                    0.00%     399ns         1     399ns     399ns     399ns  cuModuleGetLoadingMode\n",
            "                    0.00%     337ns         1     337ns     337ns     337ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./elsofeladat"
      ],
      "metadata": {
        "id": "JY3PEnL749LB",
        "outputId": "de586c1e-4e09-4995-ede9-f6147378e7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Véletlenszerű Pontok Távolságszámítás\n",
            "=========================================\n",
            "Pontok száma: 1000000\n",
            "Block méret: 256\n",
            "Blokkok száma: 3907\n",
            "Elméleti átlagos távolság: ~0.521405\n",
            "\n",
            "==10275== NVPROF is profiling process 10275, command: ./elsofeladat\n",
            "Eredmények:\n",
            "-----------\n",
            "Számított átlagos távolság: 0.765454\n",
            "Eltérés az elméleti értéktől: 0.244049\n",
            "\n",
            "Teljesítmény:\n",
            "CUDA kernel futási idő: 41.27 ms\n",
            "Teljes CPU idő: 44 ms\n",
            "Feldolgozott pontok/másodperc: 24231681\n",
            "\n",
            "Statisztikák:\n",
            "Minimális távolság: 0.002267\n",
            "Maximális távolság: 1.413326\n",
            "Tartomány: [0.002267, 1.413326]\n",
            "\n",
            "Mintaértékek (első 10 távolság):\n",
            "0.484252 0.864508 0.719313 1.323977 0.083039 0.411939 1.020200 0.805797 0.667082 0.453043 \n",
            "==10275== Profiling application: ./elsofeladat\n",
            "==10275== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "318.80ms  40.793ms           (3907 1 1)       (256 1 1)        63        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  generate_points_and_distances(float*, int, __int64) [129]\n",
            "359.65ms  1.5559ms                    -               -         -         -         -  3.8147MB  2.3943GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-summary ./elsofeladat"
      ],
      "metadata": {
        "id": "XLeiZVTw4r55",
        "outputId": "30fc1af0-31f0-48d4-db97-ce73c49d8989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Véletlenszerű Pontok Távolságszámítás\n",
            "=========================================\n",
            "Pontok száma: 1000000\n",
            "Block méret: 256\n",
            "Blokkok száma: 3907\n",
            "Elméleti átlagos távolság: ~0.521405\n",
            "\n",
            "==10364== NVPROF is profiling process 10364, command: ./elsofeladat\n",
            "Eredmények:\n",
            "-----------\n",
            "Számított átlagos távolság: 0.765454\n",
            "Eltérés az elméleti értéktől: 0.244049\n",
            "\n",
            "Teljesítmény:\n",
            "CUDA kernel futási idő: 41.38 ms\n",
            "Teljes CPU idő: 44 ms\n",
            "Feldolgozott pontok/másodperc: 24163704\n",
            "\n",
            "Statisztikák:\n",
            "Minimális távolság: 0.002267\n",
            "Maximális távolság: 1.413326\n",
            "Tartomány: [0.002267, 1.413326]\n",
            "\n",
            "Mintaértékek (első 10 távolság):\n",
            "0.484252 0.864508 0.719313 1.323977 0.083039 0.411939 1.020200 0.805797 0.667082 0.453043 \n",
            "==10364== Profiling application: ./elsofeladat\n",
            "==10364== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   96.23%  40.863ms         1  40.863ms  40.863ms  40.863ms  generate_points_and_distances(float*, int, __int64)\n",
            "                    3.77%  1.6023ms         1  1.6023ms  1.6023ms  1.6023ms  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./masodikfeladat"
      ],
      "metadata": {
        "id": "v_CY6phv5GG2",
        "outputId": "a8d0139c-a24b-4408-bf57-3dfd306036b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo π Számítás - Szekvenciális vs CUDA\n",
            "===============================================\n",
            "Valós π érték: 3.1415926536\n",
            "\n",
            "Paraméterek:\n",
            "Pontok száma: 10000000\n",
            "CUDA block méret: 256\n",
            "\n",
            "Szekvenciális eredmények:\n",
            "Pontok száma: 10000000\n",
            "Körön belüli pontok: 7853911\n",
            "π becslés: 3.1415644000\n",
            "Eltérés a valós π-től: 0.0000282536\n",
            "Futási idő: 1373 ms\n",
            "Pontok/másodperc: 7283321\n",
            "\n",
            "Block méret: 256\n",
            "Blokkok száma: 39063\n",
            "Pontok szálanként: 1\n",
            "Összes feldolgozott pont: 10000128\n",
            "==10457== NVPROF is profiling process 10457, command: ./masodikfeladat\n",
            "CUDA eredmények:\n",
            "Feldolgozott pontok: 10000128\n",
            "Körön belüli pontok: 7851674\n",
            "π becslés: 3.1406293999\n",
            "Eltérés a valós π-től: 0.0009632536\n",
            "CUDA kernel futási idő: 306.94 ms\n",
            "Pontok/másodperc: 32579695\n",
            "\n",
            "Összehasonlítás:\n",
            "================\n",
            "Szekvenciális π: 3.1415644000 (eltérés: 0.0000282536)\n",
            "CUDA π:          3.1406293999 (eltérés: 0.0009632536)\n",
            "Különbség:       0.0009350001\n",
            "\n",
            "Teljesítmény:\n",
            "=============\n",
            "Szekvenciális teljes idő: 1373 ms\n",
            "CUDA teljes idő:          713 ms\n",
            "Gyorsítás:                1.93x\n",
            "Elméleti hatékonyság:     0.19%\n",
            "\n",
            "Precizitás elemzés:\n",
            "==================\n",
            "Elméleti hiba (95% CI):   ±0.000620\n",
            "Szekvenciális hiba:       0.000028\n",
            "CUDA hiba:                0.000963\n",
            "==10457== Profiling application: ./masodikfeladat\n",
            "==10457== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   85.01%  306.48ms         1  306.48ms  306.48ms  306.48ms  monte_carlo_pi_kernel(__int64*, __int64, __int64)\n",
            "                   14.99%  54.038ms         1  54.038ms  54.038ms  54.038ms  [CUDA memcpy DtoH]\n",
            "      API calls:   55.72%  306.50ms         1  306.50ms  306.50ms  306.50ms  cudaEventSynchronize\n",
            "                   34.12%  187.67ms         1  187.67ms  187.67ms  187.67ms  cudaMalloc\n",
            "                   10.00%  55.007ms         1  55.007ms  55.007ms  55.007ms  cudaMemcpy\n",
            "                    0.08%  456.58us         1  456.58us  456.58us  456.58us  cudaLaunchKernel\n",
            "                    0.04%  216.10us         1  216.10us  216.10us  216.10us  cudaFree\n",
            "                    0.03%  141.41us       114  1.2400us     104ns  58.135us  cuDeviceGetAttribute\n",
            "                    0.01%  28.468us         2  14.234us  5.0460us  23.422us  cudaEventRecord\n",
            "                    0.00%  26.276us         2  13.138us     670ns  25.606us  cudaEventCreate\n",
            "                    0.00%  19.362us         1  19.362us  19.362us  19.362us  cudaEventElapsedTime\n",
            "                    0.00%  16.156us         1  16.156us  16.156us  16.156us  cudaDeviceSynchronize\n",
            "                    0.00%  12.679us         1  12.679us  12.679us  12.679us  cuDeviceGetName\n",
            "                    0.00%  6.3470us         2  3.1730us     999ns  5.3480us  cudaEventDestroy\n",
            "                    0.00%  5.9070us         1  5.9070us  5.9070us  5.9070us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4720us         3     490ns     124ns  1.1390us  cuDeviceGetCount\n",
            "                    0.00%     726ns         2     363ns     127ns     599ns  cuDeviceGet\n",
            "                    0.00%     604ns         1     604ns     604ns     604ns  cuModuleGetLoadingMode\n",
            "                    0.00%     457ns         1     457ns     457ns     457ns  cuDeviceTotalMem\n",
            "                    0.00%     273ns         1     273ns     273ns     273ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./masodikfeladat"
      ],
      "metadata": {
        "id": "zMIyb_PA5Jnc",
        "outputId": "e35f005b-b3be-48d9-8480-8c9bae403b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo π Számítás - Szekvenciális vs CUDA\n",
            "===============================================\n",
            "Valós π érték: 3.1415926536\n",
            "\n",
            "Paraméterek:\n",
            "Pontok száma: 10000000\n",
            "CUDA block méret: 256\n",
            "\n",
            "Szekvenciális eredmények:\n",
            "Pontok száma: 10000000\n",
            "Körön belüli pontok: 7852649\n",
            "π becslés: 3.1410596000\n",
            "Eltérés a valós π-től: 0.0005330536\n",
            "Futási idő: 1374 ms\n",
            "Pontok/másodperc: 7278020\n",
            "\n",
            "Block méret: 256\n",
            "Blokkok száma: 39063\n",
            "Pontok szálanként: 1\n",
            "Összes feldolgozott pont: 10000128\n",
            "==10528== NVPROF is profiling process 10528, command: ./masodikfeladat\n",
            "CUDA eredmények:\n",
            "Feldolgozott pontok: 10000128\n",
            "Körön belüli pontok: 7851674\n",
            "π becslés: 3.1406293999\n",
            "Eltérés a valós π-től: 0.0009632536\n",
            "CUDA kernel futási idő: 324.28 ms\n",
            "Pontok/másodperc: 30837963\n",
            "\n",
            "Összehasonlítás:\n",
            "================\n",
            "Szekvenciális π: 3.1410596000 (eltérés: 0.0005330536)\n",
            "CUDA π:          3.1406293999 (eltérés: 0.0009632536)\n",
            "Különbség:       0.0004302001\n",
            "\n",
            "Teljesítmény:\n",
            "=============\n",
            "Szekvenciális teljes idő: 1374 ms\n",
            "CUDA teljes idő:          717 ms\n",
            "Gyorsítás:                1.92x\n",
            "Elméleti hatékonyság:     0.19%\n",
            "\n",
            "Precizitás elemzés:\n",
            "==================\n",
            "Elméleti hiba (95% CI):   ±0.000620\n",
            "Szekvenciális hiba:       0.000533\n",
            "CUDA hiba:                0.000963\n",
            "==10528== Profiling application: ./masodikfeladat\n",
            "==10528== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "310.54ms  323.83ms          (39063 1 1)       (256 1 1)        63        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  monte_carlo_pi_kernel(__int64*, __int64, __int64) [129]\n",
            "634.48ms  51.304ms                    -               -         -         -         -  76.295MB  1.4522GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-summary ./masodikfeladat"
      ],
      "metadata": {
        "id": "YXP_BKtY5NZd",
        "outputId": "2d2d2987-60a5-46be-b813-e4cffdeefa9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monte Carlo π Számítás - Szekvenciális vs CUDA\n",
            "===============================================\n",
            "Valós π érték: 3.1415926536\n",
            "\n",
            "Paraméterek:\n",
            "Pontok száma: 10000000\n",
            "CUDA block méret: 256\n",
            "\n",
            "Szekvenciális eredmények:\n",
            "Pontok száma: 10000000\n",
            "Körön belüli pontok: 7853518\n",
            "π becslés: 3.1414072000\n",
            "Eltérés a valós π-től: 0.0001854536\n",
            "Futási idő: 1897 ms\n",
            "Pontok/másodperc: 5271481\n",
            "\n",
            "Block méret: 256\n",
            "Blokkok száma: 39063\n",
            "Pontok szálanként: 1\n",
            "Összes feldolgozott pont: 10000128\n",
            "==10615== NVPROF is profiling process 10615, command: ./masodikfeladat\n",
            "CUDA eredmények:\n",
            "Feldolgozott pontok: 10000128\n",
            "Körön belüli pontok: 7851674\n",
            "π becslés: 3.1406293999\n",
            "Eltérés a valós π-től: 0.0009632536\n",
            "CUDA kernel futási idő: 348.69 ms\n",
            "Pontok/másodperc: 28679490\n",
            "\n",
            "Összehasonlítás:\n",
            "================\n",
            "Szekvenciális π: 3.1414072000 (eltérés: 0.0001854536)\n",
            "CUDA π:          3.1406293999 (eltérés: 0.0009632536)\n",
            "Különbség:       0.0007778001\n",
            "\n",
            "Teljesítmény:\n",
            "=============\n",
            "Szekvenciális teljes idő: 1897 ms\n",
            "CUDA teljes idő:          785 ms\n",
            "Gyorsítás:                2.42x\n",
            "Elméleti hatékonyság:     0.24%\n",
            "\n",
            "Precizitás elemzés:\n",
            "==================\n",
            "Elméleti hiba (95% CI):   ±0.000620\n",
            "Szekvenciális hiba:       0.000185\n",
            "CUDA hiba:                0.000963\n",
            "==10615== Profiling application: ./masodikfeladat\n",
            "==10615== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   86.72%  348.19ms         1  348.19ms  348.19ms  348.19ms  monte_carlo_pi_kernel(__int64*, __int64, __int64)\n",
            "                   13.28%  53.341ms         1  53.341ms  53.341ms  53.341ms  [CUDA memcpy DtoH]\n"
          ]
        }
      ]
    }
  ]
}