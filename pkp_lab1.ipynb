{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aksza/2022.Oop/blob/master/pkp_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labor 1\n",
        "\n",
        "# Bevezetés a CUDA GPU programozásba\n",
        "\n",
        "## Mi a párhuzamos programozás?\n",
        "\n",
        "A párhuzamos programozás olyan programozási paradigma, ahol több számítási feladat egyidejűleg, egymással párhuzamosan fut. Hagyományos, szekvenciális programozás esetén a program utasításai egymás után, lépésről lépésre hajtódnak végre. Ezzel szemben a párhuzamos programozás lehetővé teszi, hogy több utasítás egyidejűleg fusson.\n",
        "\n",
        "## Miért van szükség párhuzamos programozásra?\n",
        "\n",
        "A processzorteljesítmény növelése az évek során elérte fizikai korlátait (órajel, hőtermelés). Az egymagos processzorok teljesítménye nem növelhető a végtelenségig, így többmagos processzorokat kezdtek gyártani. Bizonyos feladatok természetüknél fogva párhuzamosíthatók (pl. képfeldolgozás, mátrixműveletek, szimulációk), amelyek hatékonyan felgyorsíthatók párhuzamos feldolgozással.\n",
        "\n",
        "## GPU-k és a CUDA szerepe\n",
        "\n",
        "A grafikus processzorok (GPU-k) eredetileg a számítógépes grafikához készültek, de felismerték, hogy bizonyos számítási feladatok (különösen azok, amelyek sok adaton ugyanazt a műveletet végzik) sokkal gyorsabban futhatnak rajtuk. A GPU-k több ezer egyszerűbb számítási magot tartalmaznak, szemben a CPU-k néhány komplex magjával.\n",
        "\n",
        "A [GPGPU](https://www.gigabyte.com/Glossary/gpgpu) rövidítés a \"General Purpose Graphics Processing Unit\"-ra utal, ami egy olyan számítási technológia, amely során a grafikus kártyák (GPU-k) számítási kapacitását használjuk általános célú számítások végrehajtására. Mint láttuk, a GPU-k eredetileg kifejezetten grafikus feladatok hatékony megoldására lettek tervezve, mint például a képfeldolgozás vagy 3D grafika. Azonban, az elmúlt években a GPU-k egyre inkább beépültek az általános célú számítások világába is.\n",
        "\n",
        "A GPGPU technológia alkalmazása lehetővé teszi, hogy a GPU-k nagy számítási kapacitását felhasználják olyan feladatok elvégzésére, mint például a gépi tanulás, a kriptográfiai műveletek, a tudományos szimulációk vagy a nagy adathalmazok feldolgozása. A GPGPU technológia használata jelentősen felgyorsíthatja ezeket a számítási feladatokat, és csökkentheti azok elvégzésének idejét.\n",
        "\n",
        "Az NVIDIA 2007-ben bemutatta a CUDA (Compute Unified Device Architecture) platformot, amely lehetővé teszi a programozók számára, hogy a GPU-kat általános célú számításokra is használhassák (GPGPU - General-Purpose computing on Graphics Processing Units).\n",
        "\n",
        "## Heterogén párhuzamos programozás\n",
        "\n",
        "A CUDA programozást heterogén párhuzamos programozásnak is nevezik, mert:\n",
        "\n",
        "1. **Heterogén rendszer**: A CPU (host) és a GPU (device) együttműködve oldják meg a feladatot\n",
        "2. **Különböző architektúrák**: A CPU és a GPU eltérő architektúrával és memóriarendszerrel rendelkezik\n",
        "3. **Eltérő feladatkörök**: A CPU általában a szekvenciális részeket, a GPU a párhuzamosítható részeket hajtja végre\n",
        "\n"
      ],
      "metadata": {
        "id": "-vk-2uPuPL6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Runtime API\n",
        "\n",
        "\n",
        "A [CUDA Runtime API](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html) egy függvénykönyvtárból és a C++ szintaxis egyszerű kiegészítéséből áll.\n",
        "\n",
        "A CUDA programokat `*.cu` (nem `*.c` vagy `*.cpp`) kiterjesztésű állományokban írjuk. Mint meglátjuk, alapvetően C++ kódot írünk, melyet néha kiegészíthetünk gyorsítón futtatandó programrészletekkel, úgynevezett \"CUDA kernelekkel\".\n",
        "\n",
        "Az alábbi kódrészlet egy tökéletesen működő CUDA program, csak éppenséggel még nem tartalmaz gyorsító specifikus kódot.\n",
        "\n",
        "```cpp\n",
        "#include <iostream>\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    std::cout << \"Hello World!\" << std::endl;\n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Ha a programunkban szeretnénk kihasználni a GPU számítási kapacitását lehetőségeit is, akkor GPU kódot is írnunk kell. Ez a számításokhoz szükséges adatokat előkészíti, átmásolja, meghívja a GPU kernelt, majd az adatokat visszamásolja. A CUDA forráskódok általában keverten tartalmazzák a CPU, és GPU kódokat.\n",
        "\n",
        "A GPU-n futó számítást úgynevezett a fentebb említett kernel függvények megadásával tudjuk megvalósítani. Ezek a `__global__` előtaggal rendelkező függvények.\n",
        "\n",
        "### Kompilálás és futtatás\n",
        "\n",
        "A CUDA (`*.cu` kiterjesztésű) programjaink kompilálásához, használjuk a következő parancsot:  \n",
        "\n",
        "```\n",
        "!nvcc mysurcefile.cu -o programname\n",
        "```\n",
        "\n",
        "majd sikeres kompilálás esetében, futtatáshoz:\n",
        "```\n",
        "!./programname\n",
        "```"
      ],
      "metadata": {
        "id": "ikVGiziWBV53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA program általános szerkezete\n",
        "\n",
        "A CUDA programok speciális felépítéssel rendelkeznek, ami a heterogén rendszer sajátosságaiból adódik. Az alábbiakban megnézzük a tipikus CUDA program szerkezetét és a főbb lépéseket.\n",
        "\n",
        "## 1. Program inicializálás és feladatmeghatározás\n",
        "\n",
        "Minden CUDA program a host (CPU) oldalon kezdődik, ahol:\n",
        "- Meghatározzuk a feladat méretét (pl. feldolgozandó elemek száma)\n",
        "- Inicializáljuk a bemeneti adatokat\n",
        "- Kiszámoljuk a memória igényeket\n",
        "\n",
        "```c\n",
        "int n = 1000000;  // Elemek száma\n",
        "size_t size = n * sizeof(float);  // Szükséges memória mérete bájtokban\n",
        "```\n",
        "\n",
        "## 2. Memória kezelés a heterogén rendszerben\n",
        "\n",
        "Kulcsfontosságú megérteni, hogy a CPU és GPU külön memóriaterülettel rendelkezik, így explicit memóriakezelésre van szükség:\n",
        "\n",
        "### a) Host memória foglalás (CPU oldal)\n",
        "```c\n",
        "float *h_input = (float*)malloc(size);\n",
        "float *h_output = (float*)malloc(size);\n",
        "\n",
        "// Adatok inicializálása\n",
        "for (int i = 0; i < n; i++) {\n",
        "    h_input[i] = rand() / (float)RAND_MAX;\n",
        "}\n",
        "```\n",
        "\n",
        "### b) Device memória foglalás (GPU oldal)\n",
        "```c\n",
        "float *d_input = NULL;\n",
        "float *d_output = NULL;\n",
        "cudaMalloc((void**)&d_input, size);\n",
        "cudaMalloc((void**)&d_output, size);\n",
        "```\n",
        "\n",
        "### c) Adatok másolása host-ról device-ra\n",
        "```c\n",
        "cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "```\n",
        "\n",
        "## 3. Kernel definíció\n",
        "\n",
        "A kernel a GPU-n futó függvény, amelyet a `__global__` kulcsszóval jelölünk:\n",
        "\n",
        "```c\n",
        "__global__ void processData(float *input, float *output, int n) {\n",
        "    // Egyedi szálazonosító számítása\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "    // Ellenőrzés, hogy a szál érvényes adatelemen dolgozik-e\n",
        "    if (idx < n) {\n",
        "        // Tényleges munka elvégzése (példa: érték duplázása)\n",
        "        output[idx] = 2.0f * input[idx];\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "## 4. Végrehajtási konfiguráció és kernel indítás\n",
        "\n",
        "A kernel indításakor meg kell határozni a végrehajtási konfigurációt (hogyan szerveződnek a szálak blokkokba, a blokkok egy rácsba (grid)):\n",
        "\n",
        "```c\n",
        "// Blokkméret meghatározása (szálak száma blokkonként)\n",
        "int threadsPerBlock = 256;\n",
        "\n",
        "// Blokkok számának kiszámítása\n",
        "// Felfelé kerekítünk, hogy minden adatelemhez jusson szál\n",
        "int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "\n",
        "// Kernel indítása a megadott konfigurációval\n",
        "processData<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n",
        "\n",
        "// Aszinkron végrehajtás - megvárjuk a befejezést\n",
        "cudaDeviceSynchronize();\n",
        "```\n",
        "\n",
        "## 5. Eredmények visszaolvasása\n",
        "\n",
        "A feldolgozás után az eredményeket vissza kell másolni a GPU-ról a CPU memóriába:\n",
        "\n",
        "```c\n",
        "cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "```\n",
        "\n",
        "## 6. Eredmények feldolgozása és erőforrások felszabadítása\n",
        "\n",
        "A CPU oldalon feldolgozzuk az eredményeket, majd felszabadítjuk a lefoglalt erőforrásokat:\n",
        "\n",
        "```c\n",
        "// Eredmények feldolgozása (pl. ellenőrzés)\n",
        "for (int i = 0; i < 10; i++) {\n",
        "    printf(\"Input: %f, Output: %f\\n\", h_input[i], h_output[i]);\n",
        "}\n",
        "\n",
        "// Erőforrások felszabadítása\n",
        "cudaFree(d_input);\n",
        "cudaFree(d_output);\n",
        "free(h_input);\n",
        "free(h_output);\n",
        "```\n",
        "\n",
        "## 7. Hibakezelés (jó gyakorlat)\n",
        "\n",
        "A CUDA API hívások hibakódot adnak vissza, amit érdemes ellenőrizni:\n",
        "\n",
        "```c\n",
        "cudaError_t err = cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "if (err != cudaSuccess) {\n",
        "    printf(\"CUDA error: %s\\n\", cudaGetErrorString(err));\n",
        "    // Hibakezelés...\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hz1yiTaVTck0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Szálazonosító számítása a CUDA-ban\n",
        "\n",
        "### Egyedi szálazonosítók működése\n",
        "\n",
        "A CUDA architektúrában minden szálnak szüksége van egy egyedi azonosítóra, hogy tudja, melyik adatelemen kell dolgoznia. Ez az azonosító kiszámítása kulcsfontosságú a párhuzamos végrehajtás során. A CUDA hierarchikus modellben gondolkodik: a szálak blokkokba vannak szervezve, a blokkok pedig rácsot (grid) alkotnak.\n",
        "\n",
        "Az egyedi globális szálazonosító (global thread ID) kiszámítása a következő képlettel történik 1D szervezás esetében:\n",
        "\n",
        "```\n",
        "globalId = blockIdx.x * blockDim.x + threadIdx.x\n",
        "```\n",
        "\n",
        "Ahol:\n",
        "- `blockIdx.x`: Az aktuális blokk indexe a rácsban\n",
        "- `blockDim.x`: A blokkon belüli szálak száma (szálak per blokk)\n",
        "- `threadIdx.x`: A szál indexe a blokkon belül\n",
        "\n",
        "### Példák az 1D azonosító számításra\n",
        "\n",
        "- Ha 2 blokkunk van, egyenként 3 szállal:\n",
        "  - 0. blokk, 0. szál: `0 * 3 + 0 = 0`\n",
        "  - 0. blokk, 1. szál: `0 * 3 + 1 = 1`\n",
        "  - 0. blokk, 2. szál: `0 * 3 + 2 = 2`\n",
        "  - 1. blokk, 0. szál: `1 * 3 + 0 = 3`\n",
        "  - 1. blokk, 1. szál: `1 * 3 + 1 = 4`\n",
        "  - 1. blokk, 2. szál: `1 * 3 + 2 = 5`\n",
        "\n",
        "\n",
        "Ez a számítási modell biztosítja, hogy minden szál pontosan tudja, melyik adatelemen kell dolgoznia, függetlenül attól, hogy hány blokkot és szálat használunk a párhuzamos végrehajtáshoz.\n",
        "\n",
        "\n",
        "Késobbiekben majd megltájuk, hogy például kétdimenziós (pl. mátrixműveleteknél) szervezés esetében is tudunk mindig linearizálni, egyedi azonosítot számítani:\n",
        "\n",
        "```c\n",
        "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "int idx = row * width + col;  // Linearizált index\n",
        "```\n"
      ],
      "metadata": {
        "id": "MK-ZXdmySc9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Példa: Teljes CUDA program a vektorok összeadására\n",
        "\n",
        "Két vektor elemeinek összegzése párhuzamosan CUDA-val:\n",
        "\n",
        "```c\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// GPU kernel - minden szál egy elemet dolgoz fel\n",
        "__global__ void vectorAdd(int *a, int *b, int *c, int n) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000;\n",
        "    int size = n * sizeof(int);\n",
        "    \n",
        "    // Host memória foglalása\n",
        "    int *h_a = (int *)malloc(size);\n",
        "    int *h_b = (int *)malloc(size);\n",
        "    int *h_c = (int *)malloc(size);\n",
        "    \n",
        "    // Vektorok inicializálása\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = i;\n",
        "        h_b[i] = i;\n",
        "    }\n",
        "    \n",
        "    // Device memória foglalása\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "    \n",
        "    // Adatok másolása a host-ról a device-ra\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "    \n",
        "    // Kernel indítása 256 szállal blokkonként\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
        "    \n",
        "    // Eredmény visszamásolása a device-ról a host-ra\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "    \n",
        "    // Eredmény ellenőrzése\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "    \n",
        "    // Memória felszabadítása\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "    \n",
        "    return 0;\n",
        "}\n",
        "```\n",
        "\n",
        "A CUDA programozás alapelvei a példában fellelhetőek:\n",
        "1. Szétválasztjuk a kódot CPU (host) és GPU (device) részekre\n",
        "2. A GPU kódot speciális `__global__` kulcsszóval jelöljük (kernel)\n",
        "3. Memóriát foglalunk mindkét eszközön, és explicit másolással kommunikálunk\n",
        "4. Meghatározzuk a párhuzamosítás szintjét (hány blokk és hány szál/blokk)\n",
        "\n",
        "A CUDA előnye, hogy C nyelvi környezetben programozhatunk, minimális nyelvi kiterjesztésekkel, miközben kihasználjuk a GPU-k párhuzamos feldolgozási képességeit."
      ],
      "metadata": {
        "id": "xCxjHelCPdjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colaboratory\n",
        "\n",
        "A Google Colaboratory (röviden Colab) egy ingyenes online platform, amely lehetővé teszi a felhasználók számára a (főleg) Python programozási nyelv interaktív környezetében történő munkavégzését. Az egyik fő előnye, hogy ingyenes GPU és TPU (Tensor Processing Unit) számítási erőforrásokat biztosít a felhasználók számára, azzal a cállal, hogy lehetővé tegye a nagyobb adatméretű és bonyolultabb gépi tanulási és adatelemzési feladatok végrehajtását.\n",
        "\n",
        "Az GPU erőforrások igénybevétele érdekében, a felhasználóknak a futásidejű környezet típusát kell áttálítsa. Ehhez, a `Runtime/Change runtime` menűpontban válasszuk ki, hogy `GPU`."
      ],
      "metadata": {
        "id": "RDNbKVlU9cMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `GPU` elérhetőségét teszteln tudjuk a következő kóddal:"
      ],
      "metadata": {
        "id": "LH1eSOFi_g_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j03fCHP_PoQ",
        "outputId": "bdb2fca3-b732-4c01-ac69-997401d3c22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Nvidia kompájler verziójának lekérése:"
      ],
      "metadata": {
        "id": "SxGnDTAs_51e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hxdSMs37ZtV",
        "outputId": "99c0d5c8-8b2e-44da-8368-f7c5020bb843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A kód cellákkban csak Python kód futtatható direkt modon. Ezért, [%%writefile magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html) parancsal fogjuk kiírni a lemezre a programjainkat, majd ezeket más kódcellákban kompiláljuk és futtatjuk.\n",
        "\n",
        "Például, az alábbi példaprogram GPU-n végzi el két szám összeadását."
      ],
      "metadata": {
        "id": "6BAs6WrnAA7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vecadd.cu\n",
        "#include <cuda.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void add(int a, int b, int* c)\n",
        "{\n",
        "    *c = a + b;\n",
        "    return;\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv)\n",
        "{\n",
        "    int c;\n",
        "    int* dev_c;\n",
        "\n",
        "\t  //memória foglalás a GPU-n\n",
        "    cudaMalloc((void**)&dev_c, sizeof(int) );\n",
        "\n",
        "    add<<<1,1>>>(1, 2, dev_c);\n",
        "\n",
        "    cudaMemcpy(&c, dev_c, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"a + b = %d\\n\", c);\n",
        "\n",
        "    //lefolglat memória felszabadítása\n",
        "    cudaFree(dev_c);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL9JO5bt7hrW",
        "outputId": "461ddc26-4a29-4513-c665-d94827b01161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "És íme meg is jelenik a `vecadd.cu` állomány:"
      ],
      "metadata": {
        "id": "um6CYayyAxIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsq0-uVN9Gbo",
        "outputId": "5d9ba254-3682-48b4-8070-0a4460390efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  vecadd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kompilálás:"
      ],
      "metadata": {
        "id": "lPeatuhnA6Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vecadd.cu"
      ],
      "metadata": {
        "id": "B8gcA_rP9Lrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ellenörizzük ha megjelent a futtatható bináris program (`a.out`):"
      ],
      "metadata": {
        "id": "HBKgJTrFA-Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0qF_LNk9P_I",
        "outputId": "c0348425-f92a-46f0-c0c7-ef4051d48dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a.out  sample_data  vecadd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Futtatás:"
      ],
      "metadata": {
        "id": "0GJCibQSBJer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lngC1HH79R7N",
        "outputId": "bd752d8f-b8b4-4e7b-ed6e-ea7dab0bc818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a + b = 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feladatok\n",
        "\n",
        "1. Írjunk egy CUDA programot, mely a [`cudaError_t cudaGetDeviceCount (int * count)`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g18808e54893cfcaafefeab31a73cc55f), [`cudaError_t cudaGetDeviceProperties (struct cudaDeviceProp * prop, int device )`](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g1bf9d625a931d657e08db2b4391170f0) függvények segítségével, kiírja a CUDA kompatibilis GPU főbb paramétereit.\n",
        "A jellemzőket tartalmazó `cudaDeviceProp` struktúra definíciója a következő:\n",
        "\n",
        "  ```cpp\n",
        "  struct cudaDeviceProp {\n",
        "                char name[256];\n",
        "                cudaUUID_t uuid;\n",
        "                size_t totalGlobalMem;\n",
        "                size_t sharedMemPerBlock;\n",
        "                int regsPerBlock;\n",
        "                int warpSize;\n",
        "                size_t memPitch;\n",
        "                int maxThreadsPerBlock;\n",
        "                int maxThreadsDim[3];\n",
        "                int maxGridSize[3];\n",
        "                int clockRate;\n",
        "                size_t totalConstMem;\n",
        "                int major;\n",
        "                int minor;\n",
        "                size_t textureAlignment;\n",
        "                size_t texturePitchAlignment;\n",
        "                int deviceOverlap;\n",
        "                int multiProcessorCount;\n",
        "                int kernelExecTimeoutEnabled;\n",
        "                int integrated;\n",
        "                int canMapHostMemory;\n",
        "                int computeMode;\n",
        "                int maxTexture1D;\n",
        "                int maxTexture1DMipmap;\n",
        "                int maxTexture1DLinear;\n",
        "                int maxTexture2D[2];\n",
        "                int maxTexture2DMipmap[2];\n",
        "                int maxTexture2DLinear[3];\n",
        "                int maxTexture2DGather[2];\n",
        "                int maxTexture3D[3];\n",
        "                int maxTexture3DAlt[3];\n",
        "                int maxTextureCubemap;\n",
        "                int maxTexture1DLayered[2];\n",
        "                int maxTexture2DLayered[3];\n",
        "                int maxTextureCubemapLayered[2];\n",
        "                int maxSurface1D;\n",
        "                int maxSurface2D[2];\n",
        "                int maxSurface3D[3];\n",
        "                int maxSurface1DLayered[2];\n",
        "                int maxSurface2DLayered[3];\n",
        "                int maxSurfaceCubemap;\n",
        "                int maxSurfaceCubemapLayered[2];\n",
        "                size_t surfaceAlignment;\n",
        "                int concurrentKernels;\n",
        "                int ECCEnabled;\n",
        "                int pciBusID;\n",
        "                int pciDeviceID;\n",
        "                int pciDomainID;\n",
        "                int tccDriver;\n",
        "                int asyncEngineCount;\n",
        "                int unifiedAddressing;\n",
        "                int memoryClockRate;\n",
        "                int memoryBusWidth;\n",
        "                int l2CacheSize;\n",
        "                int persistingL2CacheMaxSize;\n",
        "                int maxThreadsPerMultiProcessor;\n",
        "                int streamPrioritiesSupported;\n",
        "                int globalL1CacheSupported;\n",
        "                int localL1CacheSupported;\n",
        "                size_t sharedMemPerMultiprocessor;\n",
        "                int regsPerMultiprocessor;\n",
        "                int managedMemory;\n",
        "                int isMultiGpuBoard;\n",
        "                int multiGpuBoardGroupID;\n",
        "                int singleToDoublePrecisionPerfRatio;\n",
        "                int pageableMemoryAccess;\n",
        "                int concurrentManagedAccess;\n",
        "                int computePreemptionSupported;\n",
        "                int canUseHostPointerForRegisteredMem;\n",
        "                int cooperativeLaunch;\n",
        "                int cooperativeMultiDeviceLaunch;\n",
        "                int pageableMemoryAccessUsesHostPageTables;\n",
        "                int directManagedMemAccessFromHost;\n",
        "                int accessPolicyMaxWindowSize;\n",
        "            }\n",
        "   ```\n",
        "\n",
        "2. Írjunk egy \"Hello, World\" CUDA kernelt, melyben minden szál kiírja az [egyedi azonosítóját](https://blog.usejournal.com/cuda-thread-indexing-fb9910cba084). Hívjuk meg a kernelt különböző rács és blokk konfigurációkkal. Használjuk a [cudaError_t cudaDeviceSynchronize ( void )](https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g10e20b05a95f638a4071a655503df25d) függvényt a kernel hívás bevárására.\n",
        "\n",
        "3. Futtasuk a két tömb összeadása példát.\n",
        "4. Készítsünk CUDA programot, amely egy vektort megszoroz egy konstans értékkel, majd megkeresi a vektor legnagyobb értékét.\n",
        "\n",
        "  **Specifikáció**:\n",
        "  - Hozzunk létre egy 100000 elemű, véletlenszámokat tartalmazó vektort\n",
        "  - A konstans szorzó értéke legyen 2.5\n",
        "  - Írjunk kernelt a vektor konstanssal való szorzásához\n",
        "  - A maximumot egyelőre a CPU oldalon keressük meg a szorzás után\n",
        "\n",
        "  **Lépések**:\n",
        "  1. Hozzunk létre és inicializáljunk egy vektort a host (CPU) oldalon\n",
        "  2. Foglaljunk memóriát a device (GPU) oldalon\n",
        "  3. Másoljuk az adatokat a host-ról a device-ra\n",
        "  4. Implementáljunk és indítsunk egy kernelt a vektor szorzásához\n",
        "  5. Másoljuk vissza az eredményt a device-ról a host-ra\n",
        "  6. Keressük meg a vektor legnagyobb értékét a CPU-n\n",
        "  7. Írjuk ki az eredményt és szabadítsuk fel az erőforrásokat\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xnd_G4H09VNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helloworld.cu\n",
        "#include <cuda.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define NUM_BLOCKS 2\n",
        "#define THREADS_PER_BLOCK 8\n",
        "\n",
        "// CUDA kernel: minden szál azonosítóját kiírja\n",
        "__global__ void helloCUDA() {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    printf(\"Hello from thread %d (block %d, thread %d)\\n\", idx, blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Kernel indítása\n",
        "    helloCUDA<<<NUM_BLOCKS, THREADS_PER_BLOCK>>>();\n",
        "\n",
        "    // CUDA műveletek szinkronizálása a printf kiírásához\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "m110qJJ0QhKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06069eb7-2675-465b-c846-9354d6fe9504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting helloworld.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 helloworld.cu"
      ],
      "metadata": {
        "id": "kKwXbkZBwdIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./a.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXi5112XyBVC",
        "outputId": "1ad0624d-8c3d-457f-dc72-47f5b0f36035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from thread 0 (block 0, thread 0)\n",
            "Hello from thread 1 (block 0, thread 1)\n",
            "Hello from thread 2 (block 0, thread 2)\n",
            "Hello from thread 3 (block 0, thread 3)\n",
            "Hello from thread 4 (block 0, thread 4)\n",
            "Hello from thread 5 (block 0, thread 5)\n",
            "Hello from thread 6 (block 0, thread 6)\n",
            "Hello from thread 7 (block 0, thread 7)\n",
            "Hello from thread 8 (block 1, thread 0)\n",
            "Hello from thread 9 (block 1, thread 1)\n",
            "Hello from thread 10 (block 1, thread 2)\n",
            "Hello from thread 11 (block 1, thread 3)\n",
            "Hello from thread 12 (block 1, thread 4)\n",
            "Hello from thread 13 (block 1, thread 5)\n",
            "Hello from thread 14 (block 1, thread 6)\n",
            "Hello from thread 15 (block 1, thread 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SgYfnOq1Zdi",
        "outputId": "89f42a74-f778-4ee3-9ff9-2c11a4e29aae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Tue Feb 25 15:59:38 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0             31W /   70W |     102MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorsAdd.cu\n",
        "# include <stdio.h>\n",
        "# include <cuda_runtime.h>\n",
        "\n",
        "// GPU kernel - minden szál egy elemet dolgoz fel\n",
        "__global__ void vectorAdd(int *a, int *b, int *c, int n) {\n",
        "    int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < n) {\n",
        "        c[i] = a[i] + b[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 1000;\n",
        "    int size = n * sizeof(int);\n",
        "\n",
        "    // Host memória foglalása\n",
        "    int *h_a = (int *)malloc(size);\n",
        "    int *h_b = (int *)malloc(size);\n",
        "    int *h_c = (int *)malloc(size);\n",
        "\n",
        "    // Vektorok inicializálása\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_a[i] = i;\n",
        "        h_b[i] = i;\n",
        "    }\n",
        "\n",
        "    // Device memória foglalása\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, size);\n",
        "    cudaMalloc(&d_b, size);\n",
        "    cudaMalloc(&d_c, size);\n",
        "\n",
        "    // Adatok másolása a host-ról a device-ra\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Kernel indítása 256 szállal blokkonként\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
        "\n",
        "    // Eredmény visszamásolása a device-ról a host-ra\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Eredmény ellenőrzése\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"%d + %d = %d\\n\", h_a[i], h_b[i], h_c[i]);\n",
        "    }\n",
        "\n",
        "    // Memória felszabadítása\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoJqJcVr2EJM",
        "outputId": "ccae4c20-bb06-4005-f54f-4610f5b492b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vectorsAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 vectorsAdd.cu -o vectorsAdd"
      ],
      "metadata": {
        "id": "wGz5y8Gs2aI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vectorsAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Eqyv5jL2iIe",
        "outputId": "b76badcb-5499-4633-8cf2-d67b1f31d1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 + 0 = 0\n",
            "1 + 1 = 2\n",
            "2 + 2 = 4\n",
            "3 + 3 = 6\n",
            "4 + 4 = 8\n",
            "5 + 5 = 10\n",
            "6 + 6 = 12\n",
            "7 + 7 = 14\n",
            "8 + 8 = 16\n",
            "9 + 9 = 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorMulti.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define VECTOR_SIZE 100000   // Vektor mérete\n",
        "#define SCALAR 2.5           // Szorzó konstans\n",
        "#define THREADS_PER_BLOCK 256  // Szálak száma blokkonként\n",
        "\n",
        "// CUDA kernel a vektor elemeinek megszorzására egy konstanssal\n",
        "__global__ void multiplyByScalar(float* d_vec, float scalar, int size) {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < size) {\n",
        "        d_vec[idx] *= scalar;\n",
        "    }\n",
        "}\n",
        "\n",
        "// CPU függvény a maximum keresésére\n",
        "float findMaxCPU(float* vec, int size) {\n",
        "    float maxVal = vec[0];\n",
        "    for (int i = 1; i < size; i++) {\n",
        "        if (vec[i] > maxVal) {\n",
        "            maxVal = vec[i];\n",
        "        }\n",
        "    }\n",
        "    return maxVal;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float* h_vec = (float*)malloc(VECTOR_SIZE * sizeof(float));\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < VECTOR_SIZE; i++) {\n",
        "        h_vec[i] = (float)(rand() % 1000) / 10.0f;\n",
        "    }\n",
        "\n",
        "    float* d_vec;\n",
        "    cudaMalloc((void**)&d_vec, VECTOR_SIZE * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_vec, h_vec, VECTOR_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int numBlocks = (VECTOR_SIZE + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
        "    multiplyByScalar<<<numBlocks, THREADS_PER_BLOCK>>>(d_vec, SCALAR, VECTOR_SIZE);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(h_vec, d_vec, VECTOR_SIZE * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    float maxVal = findMaxCPU(h_vec, VECTOR_SIZE);\n",
        "\n",
        "    printf(\"Maximum érték: %.2f\\n\", maxVal);\n",
        "\n",
        "    free(h_vec);\n",
        "    cudaFree(d_vec);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AkHQN4d3NgW",
        "outputId": "9ab44af2-96b9-4a2f-ead7-6b41099ab199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vectorMulti.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 vectorMulti.cu -o vectorMulti"
      ],
      "metadata": {
        "id": "N5F9ZTvL4ohP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vectorMulti"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C38YmT7544ru",
        "outputId": "2280e816-9609-4d78-ab07-8d05c71ac03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum érték: 249.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kiir.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main() {\n",
        "    int deviceCount = 0;\n",
        "    cudaError_t err = cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "    if (err != cudaSuccess) {\n",
        "        printf(\"Error getting device count: %s\\n\", cudaGetErrorString(err));\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    printf(\"Number of CUDA-compatible devices: %d\\n\", deviceCount);\n",
        "\n",
        "    for (int i = 0; i < deviceCount; i++) {\n",
        "        struct cudaDeviceProp prop;\n",
        "        cudaGetDeviceProperties(&prop, i);\n",
        "\n",
        "        // Kiírjuk a GPU jellemzőit\n",
        "        printf(\"\\nDevice %d:\\n\", i);\n",
        "        printf(\"  Name: %s\\n\", prop.name);\n",
        "        printf(\"  Total Global Memory: %zu bytes\\n\", prop.totalGlobalMem);\n",
        "        printf(\"  Shared Memory per Block: %zu bytes\\n\", prop.sharedMemPerBlock);\n",
        "        printf(\"  Registers per Block: %d\\n\", prop.regsPerBlock);\n",
        "        printf(\"  Warp Size: %d\\n\", prop.warpSize);\n",
        "        printf(\"  Max Threads per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "        printf(\"  Max Threads Dimension: [%d, %d, %d]\\n\", prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]);\n",
        "        printf(\"  Max Grid Size: [%d, %d, %d]\\n\", prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);\n",
        "        printf(\"  Clock Rate: %d kHz\\n\", prop.clockRate);\n",
        "        printf(\"  Total Constant Memory: %zu bytes\\n\", prop.totalConstMem);\n",
        "        printf(\"  Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
        "        printf(\"  Multi Processor Count: %d\\n\", prop.multiProcessorCount);\n",
        "        printf(\"  PCI Bus ID: %d\\n\", prop.pciBusID);\n",
        "        printf(\"  PCI Device ID: %d\\n\", prop.pciDeviceID);\n",
        "        printf(\"  Memory Clock Rate: %d kHz\\n\", prop.memoryClockRate);\n",
        "        printf(\"  Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT-gyaKk5ZUx",
        "outputId": "cd8d153b-3fad-46ad-d60d-a146cc061c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing kiir.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 kiir.cu -o kiir"
      ],
      "metadata": {
        "id": "hAxpE1B15e4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./kiir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdUpWd5f5jVi",
        "outputId": "6c3cbd73-8592-416b-e3d6-4bbf76fe9b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of CUDA-compatible devices: 1\n",
            "\n",
            "Device 0:\n",
            "  Name: Tesla T4\n",
            "  Total Global Memory: 15828320256 bytes\n",
            "  Shared Memory per Block: 49152 bytes\n",
            "  Registers per Block: 65536\n",
            "  Warp Size: 32\n",
            "  Max Threads per Block: 1024\n",
            "  Max Threads Dimension: [1024, 1024, 64]\n",
            "  Max Grid Size: [2147483647, 65535, 65535]\n",
            "  Clock Rate: 1590000 kHz\n",
            "  Total Constant Memory: 65536 bytes\n",
            "  Compute Capability: 7.5\n",
            "  Multi Processor Count: 40\n",
            "  PCI Bus ID: 0\n",
            "  PCI Device ID: 4\n",
            "  Memory Clock Rate: 5001000 kHz\n",
            "  Memory Bus Width: 256 bits\n"
          ]
        }
      ]
    }
  ]
}